{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b255a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\phobert_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import json \n",
    "from collections import OrderedDict\n",
    "from underthesea import sent_tokenize\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import OrderedDict\n",
    "\n",
    "from rag_model.model.RE.final_re import *\n",
    "from rag_model.model.NER.final_ner import *\n",
    "from shared_functions.gg_sheet_drive import *\n",
    "from shared_functions.global_functions import *\n",
    "from rag_model.model.Final_pipeline.final_relation_extractor import *\n",
    "from rag_model.model.Final_pipeline.final_doc_processor import *\n",
    "\n",
    "with open('D:/Study/Education/Projects/Group_Project/rag_model/model/RE/artifact/id2relation.json', 'r') as f:\n",
    "    id2relation = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf9e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vncorenlp import VnCoreNLP\n",
    "\n",
    "ner_annotator = None\n",
    "# ner_annotator = VnCoreNLP(\"D:/Study/Education/Projects/Group_Project/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g') \n",
    "\n",
    "ner = NER(\n",
    "    model_path=\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/artifact/model_bilstm_crf.pt\",\n",
    "    token2idx_path=\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/artifact/token2idx.json\",\n",
    "    label2idx_path=\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/artifact/label2idx.json\",\n",
    "    annotator = ner_annotator\n",
    ")\n",
    "\n",
    "re_model = RE(checkpoint = 'D:/Study/Education/Projects/Group_Project/rag_model/model/RE/artifact/re_8_train_phobert_1_3.pth',\n",
    "           use_phobert=True, id2relation=id2relation, encoder_layer=1, decoder_layer=3, use_rel_pos=False, freeze_train=True) #match the model configuration\n",
    "\n",
    "final_re = Extractor(ner, re_model)\n",
    "\n",
    "processor = Doc_processor(ner, re_model, final_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ee4e2",
   "metadata": {},
   "source": [
    "### Support function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9676eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_mask = ['luáº­t', 'thÃ´ng', 'nghá»‹', 'hiáº¿n', 'quyáº¿t', 'Ä‘á»‹nh', 'phÃ¡p', 'tÆ°', 'Ä‘iá»u', 'má»¥c', 'pháº§n', 'khoáº£n', 'Ä‘iá»ƒm']\n",
    "\n",
    "def final_relation_check(text, df):\n",
    "    re_result = re_model.predict(text)\n",
    "    ner_result = ner.extract_document_metadata(text)\n",
    "\n",
    "    # Safety checks\n",
    "    if re_result is None or 'Span' not in re_result.columns or re_result['Span'].isna().all():\n",
    "        return df\n",
    "\n",
    "    # Get a clean span string\n",
    "    span = str(re_result['Span'].iloc[0]).lower()\n",
    "    span_tokens = re.findall(r'\\w+', span)\n",
    "\n",
    "    # Rule check\n",
    "    if any(token in check_mask for token in span_tokens):\n",
    "        meta = ner_result[['issue_date', 'title', 'document_id', 'document_type']].iloc[:1].reset_index(drop=True)\n",
    "        rel = re_result.iloc[:1].reset_index(drop=True)\n",
    "        combined = pd.concat([rel, meta], axis=1)\n",
    "        df = pd.concat([df, combined], ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25273133",
   "metadata": {},
   "source": [
    "#### extract_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5151104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text):\n",
    "    sentences = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        if buffer:\n",
    "            buffer += \" \" + line\n",
    "        else:\n",
    "            buffer = line\n",
    "\n",
    "        # If the current line ends with ';', sentence is complete\n",
    "        if line.endswith(';'):\n",
    "            sentences.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "\n",
    "    # Append leftover if it doesnâ€™t end with ;\n",
    "    if buffer:\n",
    "        sentences.append(buffer.strip())\n",
    "\n",
    "    # Drop everything before the first \"CÄƒn cá»©\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        if \"CÄƒn cá»©\" in s:\n",
    "            idx = s.find(\"CÄƒn cá»©\")\n",
    "            sentences[i] = s[idx:].strip()\n",
    "            sentences = sentences[i:]\n",
    "            break\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595d764",
   "metadata": {},
   "source": [
    "#### Final relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa45aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_relation(text):\n",
    "    check_mask = ['luáº­t', 'thÃ´ng', 'nghá»‹', 'hiáº¿n', 'quyáº¿t', 'Ä‘á»‹nh', 'phÃ¡p', 'tÆ°', 'Ä‘iá»u', 'má»¥c', 'pháº§n', 'khoáº£n', 'Ä‘iá»ƒm']\n",
    "\n",
    "    # Take only the first sentence\n",
    "    first_sent = sent_tokenize(text)[0]\n",
    "    sents = extract_sentences(first_sent)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Text', 'Self Root', 'Relation', 'Span', 'issue_date', 'title', 'document_id', 'document_type'])\n",
    "\n",
    "    # Proper filtering loop\n",
    "    for sent in sents:\n",
    "        df_meta = ner.extract_document_metadata(sent)\n",
    "            # check if any keyword in check_mask appears in the sentence\n",
    "        if (any(token in sent.lower() for token in check_mask)) and ((len(df_meta['document_id'].iloc[0]) > 0) or ('nÃ y' in sent.split())):\n",
    "            df = final_relation_check(sent, df)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a48063",
   "metadata": {},
   "source": [
    "#### Try ID extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6bc0e",
   "metadata": {},
   "source": [
    "#### parse legal ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dfad0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def to_roman(num):\n",
    "    val = [1000, 900, 500, 400, 100, 90, 50, 40, 10, 9, 5, 4, 1]\n",
    "    syms = [\"M\", \"CM\", \"D\", \"CD\", \"C\", \"XC\", \"L\", \"XL\", \"X\", \"IX\", \"V\", \"IV\", \"I\"]\n",
    "    roman = \"\"\n",
    "    i = 0\n",
    "    while num > 0:\n",
    "        for _ in range(num // val[i]):\n",
    "            roman += syms[i]\n",
    "            num -= val[i]\n",
    "        i += 1\n",
    "    return roman\n",
    "\n",
    "def parse_legal_ref(text, root=None):\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    lower_text = text.lower()\n",
    "\n",
    "    # Order from high â†’ low\n",
    "    hierarchy = [\"doc\", \"chapter\", \"C\", \"P\", \"SP\", \"SSP\", \"SSSP\"]\n",
    "    \n",
    "    map_type = {\n",
    "        \"chapter\": \"Chapter\",\n",
    "        \"C\": \"Clause\",\n",
    "        \"P\": \"Point\",\n",
    "        \"SP\": \"Subpoint\",\n",
    "        \"SSP\": \"Subsubpoint\",\n",
    "        \"SSSP\": \"Subsubsubpoint\"\n",
    "    }\n",
    "\n",
    "    #-ğŸ 1) Parse existing root structure-\n",
    "    existing = {key: None for key in hierarchy}\n",
    "    lowest_level_index = -1\n",
    "\n",
    "    if root:\n",
    "        root = re.sub(r'(.*)_doc_\\1', r'\\1', root)\n",
    "        \n",
    "        parts = root.split(\"_\")\n",
    "        for p in parts:\n",
    "            if p.startswith(\"SSSP_\"):\n",
    "                existing[\"SSSP\"] = p[5:]\n",
    "            elif p.startswith(\"SSP_\"):\n",
    "                existing[\"SSP\"] = p[4:]\n",
    "            elif p.startswith(\"SP_\"):\n",
    "                existing[\"SP\"] = p[3:]\n",
    "            elif p.startswith(\"P_\"):\n",
    "                existing[\"P\"] = p[2:]\n",
    "            elif p.startswith(\"C_\"):\n",
    "                existing[\"C\"] = p[2:]\n",
    "            elif p.startswith(\"chapter_\"):\n",
    "                existing[\"chapter\"] = p.split(\"chapter_\")[1]\n",
    "            elif existing[\"doc\"] is None:  # First non-prefixed part = doc\n",
    "                existing[\"doc\"] = p\n",
    "\n",
    "        for i, k in enumerate(hierarchy):\n",
    "            if existing[k] is not None:\n",
    "                lowest_level_index = i\n",
    "\n",
    "    # 2) Parse new information from text\n",
    "    result = {k: None for k in hierarchy}\n",
    "    result[\"SP\"], result[\"SSP\"], result[\"SSSP\"] = [], [], []\n",
    "\n",
    "    # Document ID (only if no root)\n",
    "    if root is None:\n",
    "        m = re.search(r\"sá»‘\\s*([A-Za-z0-9/.\\-ÄÄ‘]+)\", text)\n",
    "        if m:\n",
    "            result[\"doc\"] = m.group(1).upper()\n",
    "\n",
    "    # Chapter\n",
    "    if m := re.search(r\"chÆ°Æ¡ng\\s*(\\d+)\", lower_text):\n",
    "        result[\"chapter\"] = to_roman(int(m.group(1)))\n",
    "\n",
    "    # Clause\n",
    "    if m := re.search(r\"Ä‘iá»u\\s*(\\d+)\", lower_text):\n",
    "        result[\"C\"] = m.group(1)\n",
    "\n",
    "    # Point\n",
    "    if m := re.search(r\"khoáº£n\\s*(\\d+)\", lower_text):\n",
    "        result[\"P\"] = m.group(1)\n",
    "\n",
    "    # Subpoints and deeper levels\n",
    "    for match in re.findall(r\"Ä‘iá»ƒm\\s*([a-z](?:\\.\\d+)*)\", lower_text):\n",
    "        depth = match.count(\".\") + 1\n",
    "        key = {1: \"SP\", 2: \"SSP\", 3: \"SSSP\"}.get(depth, \"SP\")\n",
    "        result[key].append(match)\n",
    "\n",
    "    # 3) Build final structure safely\n",
    "    final = []\n",
    "\n",
    "    if root:\n",
    "        parts = root.split(\"_\")\n",
    "        final = parts[1:] if parts[0] == \"doc\" else parts  \n",
    "    # else:\n",
    "    #     # if result[\"doc\"]:\n",
    "    #     #     final.append(result[\"doc\"])\n",
    "    #     if result[\"chapter\"]:\n",
    "    #         final.append(f\"chapter_{result['chapter']}\")\n",
    "    #     if result[\"C\"]:\n",
    "    #         final.append(f\"C_{result['C']}\")\n",
    "\n",
    "    # Append deeper levels than existing\n",
    "    for i, level in enumerate(hierarchy):\n",
    "        if i <= lowest_level_index:\n",
    "            continue  # Keep hierarchy stable\n",
    "\n",
    "        val = result[level]\n",
    "        if val:\n",
    "            if level in [\"SP\", \"SSP\", \"SSSP\"]:\n",
    "                for v in val:\n",
    "                    final.append(f\"{level}_{v}\")\n",
    "            else:\n",
    "                final.append(f\"{level}_{val}\" if i > 0 else f\"{val}\")\n",
    "\n",
    "    #4) Determine node type-\n",
    "    last = final[-1]\n",
    "    prefix = last.split(\"_\")[0]\n",
    "    node_type = map_type.get(prefix, \"Document\")\n",
    "\n",
    "    return \"_\".join(final), node_type\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dbf14",
   "metadata": {},
   "source": [
    "#### extract multiple entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_word = {\n",
    "    'chapter': re.compile(r'chÆ°Æ¡ng\\s*([ivxlcdm\\d]+)', re.IGNORECASE),\n",
    "    'clause':  re.compile(r'Ä‘iá»u\\s*(\\d+)', re.IGNORECASE),\n",
    "    'point':   re.compile(r'khoáº£n\\s*(\\d+)', re.IGNORECASE),\n",
    "    'subpoint':re.compile(r'Ä‘iá»ƒm\\s*([a-z])', re.IGNORECASE),\n",
    "    'subsubpoint':re.compile(r'Ä‘iá»ƒm\\s*([a-z]\\.\\d+)', re.IGNORECASE),\n",
    "    'subsubsubpoint':re.compile(r'Ä‘iá»ƒm\\s*([a-z]\\.\\d+\\.\\d+)', re.IGNORECASE)\n",
    "}\n",
    "\n",
    "mapping = {'chapter': 'chÆ°Æ¡ng', 'clause': 'Ä‘iá»u', 'point': 'khoáº£n', 'subpoint': 'Ä‘iá»ƒm', 'subsubpoint': 'Ä‘iá»ƒm', 'subsubsubpoint': 'Ä‘iá»ƒm'}\n",
    "\n",
    "splitting_char = re.compile(r'\\s*(,|\\bvÃ \\b|\\bhoáº·c\\b)\\s*', re.IGNORECASE)\n",
    "hierarchy_range = re.compile(r'(Ä‘iá»u|khoáº£n|Ä‘iá»ƒm|chÆ°Æ¡ng)\\s*(\\d+|[a-z])\\s*Ä‘áº¿n\\s*\\1\\s*(\\d+|[a-z])', re.IGNORECASE)\n",
    "number = re.compile(r'^\\d+$')\n",
    "word = re.compile(r'^[a-z]$')\n",
    "\n",
    "def expand_ranges(text):\n",
    "    # Handle ranges like \"Äiá»u 5 Ä‘áº¿n Äiá»u 10\" or \"khoáº£n a Ä‘áº¿n khoáº£n d\"\n",
    "    \n",
    "    def repl(m):\n",
    "        lvl, s, e = m.group(1).lower(), m.group(2), m.group(3) #capture same level hierarchy word and their values\n",
    "        if s.isdigit() and e.isdigit():\n",
    "            s, e = int(s), int(e)\n",
    "            return ', '.join(f'{lvl} {i}' for i in range(s, e + 1))\n",
    "        elif s.isalpha() and e.isalpha():\n",
    "            return ', '.join(f'{lvl} {chr(i)}' for i in range(ord(s), ord(e) + 1))\n",
    "        return m.group(0)\n",
    "    return hierarchy_range.sub(repl, text)\n",
    "\n",
    "def extract_entities(text): \n",
    "    '''\n",
    "    Extract multiple entities from a multi-entities sentence in a raw text format\n",
    "    '''\n",
    "    check = ['Ä‘iá»u', 'khoáº£n', 'Ä‘iá»ƒm']\n",
    "    \n",
    "    try_text = text.lower().split()\n",
    "    if not any(word in try_text for word in check):\n",
    "        return text\n",
    "\n",
    "    else:\n",
    "        text = text.lower().strip()\n",
    "        text = expand_ranges(text)\n",
    "        # Split with capture so we can detect separators directly\n",
    "        levels = ['chapter', 'clause', 'point', 'subpoint', 'subsubpoint', 'subsubsubpoint'] if 'chÆ°Æ¡ng' in text else ['clause', 'point', 'subpoint', 'subsubpoint', 'subsubsubpoint']\n",
    "\n",
    "        # Split with capture so we can detect separators directly\n",
    "        tokens = splitting_char.split(text)\n",
    "        segments = []\n",
    "        for i in range(0, len(tokens), 2):\n",
    "            seg = tokens[i].strip()\n",
    "            sep = tokens[i+1].strip() if i+1 < len(tokens) else None\n",
    "            segments.append((seg, sep))\n",
    "\n",
    "        results, last_levels = [], {lvl: None for lvl in levels}\n",
    "        last_anchor_level = None\n",
    "\n",
    "        for seg, sep in segments:\n",
    "            if not seg:\n",
    "                continue\n",
    "\n",
    "            # detect anchors\n",
    "            anchors_found = {}\n",
    "            for lvl in levels:\n",
    "                matches = hierarchy_word[lvl].findall(seg)\n",
    "                if matches:\n",
    "                    val = matches[-1]\n",
    "                    if lvl in ('clause', 'point'):\n",
    "                        try: val = int(val)\n",
    "                        except: pass\n",
    "                    anchors_found[lvl] = val\n",
    "\n",
    "            # If segment has anchor(s)\n",
    "            if anchors_found:\n",
    "                entity = deepcopy(last_levels)\n",
    "                for lvl in levels:\n",
    "                    if lvl in anchors_found:\n",
    "                        entity[lvl] = anchors_found[lvl]\n",
    "                        # clear lower levels\n",
    "                        for l in levels[levels.index(lvl)+1:]:\n",
    "                            entity[l] = None\n",
    "                results.append({k: v for k, v in entity.items() if v is not None})\n",
    "                last_anchor_level = next(iter(anchors_found))\n",
    "                for lvl in levels:\n",
    "                    if entity.get(lvl) is not None:\n",
    "                        last_levels[lvl] = entity[lvl]\n",
    "\n",
    "            else:\n",
    "                # bare number or letter segment\n",
    "                tokens2 = re.split(r'\\s+', seg)\n",
    "                for t in tokens2:\n",
    "                    if not t:\n",
    "                        continue\n",
    "                    if number.match(t):\n",
    "                        assign_level = last_anchor_level or levels[0]\n",
    "                        entity = deepcopy(last_levels)\n",
    "                        val = int(t)\n",
    "                        entity[assign_level] = val\n",
    "                        for l in levels[levels.index(assign_level)+1:]:\n",
    "                            entity[l] = None\n",
    "                        results.append({k: v for k, v in entity.items() if v is not None})\n",
    "                        last_levels[assign_level] = val\n",
    "                    elif word.match(t): #and last_anchor_level in ['subpoint', 'subsubpoint', 'subsubsubpoint']:\n",
    "                        assign_level = 'subpoint' if 'subpoint' in levels else levels[-1]\n",
    "                        entity = deepcopy(last_levels)\n",
    "                        entity[assign_level] = t\n",
    "                        results.append({k: v for k, v in entity.items() if v is not None})\n",
    "                        last_levels[assign_level] = t\n",
    "                        last_anchor_level = assign_level\n",
    "\n",
    "            # Reset context if highest-level entity ends with a splitting character\n",
    "            if sep and any(lvl in anchors_found for lvl in ('chapter', 'clause')):\n",
    "                for lvl in levels:\n",
    "                    last_levels[lvl] = None\n",
    "                last_anchor_level = None\n",
    "        \n",
    "        # Fix missing higher-level linkage (lookahead propagation)\n",
    "        final_results = []\n",
    "        for i, e in enumerate(results):\n",
    "            # if a sub-level exists without its parent\n",
    "            if 'subsubsubpoint' in e or 'subsubpoint' in e or 'subpoint' in e:\n",
    "                if 'point' not in e or 'clause' not in e:\n",
    "                    for j in range(i + 1, len(results)):\n",
    "                        future = results[j]\n",
    "                        if 'point' in future and 'point' not in e:\n",
    "                            e['point'] = future['point']\n",
    "                        if 'clause' in future and 'clause' not in e:\n",
    "                            e['clause'] = future['clause']\n",
    "                        # stop once weâ€™ve filled both\n",
    "                        if 'clause' in e and 'point' in e:\n",
    "                            break\n",
    "\n",
    "            # if a point exists but no clause, link to next clause\n",
    "            elif 'point' in e and 'clause' not in e:\n",
    "                for j in range(i + 1, len(results)):\n",
    "                    future = results[j]\n",
    "                    if 'clause' in future:\n",
    "                        e['clause'] = future['clause']\n",
    "                        break\n",
    "\n",
    "            final_results.append(e)\n",
    "\n",
    "        map_list = []    \n",
    "        \n",
    "        df_meta = ner.extract_document_metadata(text)\n",
    "        doc_id = df_meta['document_id'].iloc[0] if df_meta['document_id'] is not None else None\n",
    "        \n",
    "        for pair in final_results:\n",
    "            result = ''\n",
    "            for key, value in pair.items():\n",
    "                if key in mapping:\n",
    "                    temp = f'{mapping[key]} {str(value)}'\n",
    "                    \n",
    "                result += f'{temp} '\n",
    "                \n",
    "            result += f'vÄƒn báº£n sá»‘ {doc_id}' if doc_id else ''\n",
    "                    \n",
    "            map_list.append(result.strip())\n",
    "            \n",
    "        return map_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e580973c",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c60f2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relation_entities(text, root = None): # Root works as root ID for \"nÃ y\" cases\n",
    "    '''\n",
    "    Return the self-root, relation type and list of second entity IDs\n",
    "    root: input root node id for \"nÃ y\" cases to be filled\n",
    "    '''\n",
    "    text = text.lower().strip()\n",
    "    df_relation = final_relation(text)\n",
    "    self_root = df_relation['Self Root'].iloc[0] if not df_relation.empty else None\n",
    "    relation = df_relation['Relation'].iloc[0] if not df_relation.empty else None\n",
    "    info = ner.extract_document_metadata(text)\n",
    "    \n",
    "    if len(df_relation) > 0:\n",
    "        if info['document_id'] is not None:\n",
    "            span = df_relation['Span'].iloc[0] if not df_relation.empty else None\n",
    "            entities = extract_entities(text)\n",
    "            mapped_entities = []\n",
    "            if len(entities) > 0:\n",
    "                for ent in entities:\n",
    "                    parsed_ref, ref_type = parse_legal_ref(ent, root)\n",
    "                    mapped_entities.append({parsed_ref:ref_type})\n",
    "            else: \n",
    "                mapped_entities.append({root:''})\n",
    "\n",
    "            return self_root, relation, mapped_entities\n",
    "        return None, None, None\n",
    "    \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0526b34",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75d1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ' Äiá»u 32, 33, 34, 51, 138 vÃ  139, Ä‘iá»ƒm b.1.1) khoáº£n 1 Äiá»u 163, khoáº£n 3 vÃ  khoáº£n 4 Äiá»u 179 cá»§a Luáº­t thi hÃ nh Ã¡n dÃ¢n sá»± sá»‘ 26/2008/QH12'\n",
    "\n",
    "# text = 'Äiá»u 63 vÃ  khoáº£n 2 Äiá»u 81 cá»§a Luáº­t Äáº¥t Ä‘ai Ä‘Æ°á»£c thay báº±ng cá»¥m tá»« â€œ dá»± Ã¡n Ä‘áº§u tÆ° â€'\n",
    "\n",
    "# text = 'Äiá»u 67 Ä‘áº¿n Äiá»u 78 vÃ  Äiá»u 105 cá»§a Luáº­t HÃ´n nhÃ¢n vÃ  gia Ä‘Ã¬nh sá»‘ 22/2000/QH10'\n",
    "\n",
    "# text = 'khoáº£n 2 Äiá»u 27, khoáº£n 3 Äiá»u 29, Äiá»u 31, khoáº£n 3 Äiá»u 32 vÃ  Äiá»u 85 cá»§a Luáº­t ban hÃ nh vÄƒn báº£n quy pháº¡m phÃ¡p luáº­t'\n",
    "\n",
    "text = 'Ä‘iá»ƒm a Ä‘áº¿n Ä‘iá»ƒm e Ä‘iá»u 32 hoáº·c Ä‘iá»u 35, Ä‘iá»ƒm b.1.1), Ä‘iá»ƒm c.1), Ä‘iá»ƒm d.1) hoáº·c Ä‘iá»ƒm e.2) khoáº£n 2 Ä‘iá»u 1, khoáº£n 3 Ä‘iá»u 10 luáº­t sá»‘ 20/2019/QH14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7200561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä‘iá»ƒm a Ä‘iá»u 32 khoáº£n 2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_32_P_2_SP_a', 'Subpoint')\n",
      "Ä‘iá»ƒm b Ä‘iá»u 32 khoáº£n 2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_32_P_2_SP_b', 'Subpoint')\n",
      "Ä‘iá»ƒm c Ä‘iá»u 32 khoáº£n 2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_32_P_2_SP_c', 'Subpoint')\n",
      "Ä‘iá»ƒm d Ä‘iá»u 32 khoáº£n 2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_32_P_2_SP_d', 'Subpoint')\n",
      "Ä‘iá»u 32 Ä‘iá»ƒm e khoáº£n 2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_32_P_2_SP_e', 'Subpoint')\n",
      "Ä‘iá»u 35 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_35', 'Clause')\n",
      "Ä‘iá»ƒm b Ä‘iá»ƒm b.1 Ä‘iá»ƒm b.1.1 khoáº£n 2 Ä‘iá»u 1 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_1_P_2_SP_b_SSP_b.1_SSSP_b.1.1', 'Subsubsubpoint')\n",
      "Ä‘iá»ƒm c Ä‘iá»ƒm c.1 khoáº£n 2 Ä‘iá»u 1 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_1_P_2_SP_c_SSP_c.1', 'Subsubpoint')\n",
      "Ä‘iá»ƒm d Ä‘iá»ƒm d.1 khoáº£n 2 Ä‘iá»u 1 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_1_P_2_SP_d_SSP_d.1', 'Subsubpoint')\n",
      "Ä‘iá»u 1 khoáº£n 2 Ä‘iá»ƒm e Ä‘iá»ƒm e.2 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_1_P_2_SP_e_SSP_e.2', 'Subsubpoint')\n",
      "Ä‘iá»u 10 khoáº£n 3 vÄƒn báº£n sá»‘ 20/2019/QH14 ('20/2019/QH14_C_10_P_3', 'Point')\n"
     ]
    }
   ],
   "source": [
    "list_ent = extract_entities(text)\n",
    "for ent in list_ent:\n",
    "    print(ent, parse_legal_ref(ent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62433dd6",
   "metadata": {},
   "source": [
    "### Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14437959",
   "metadata": {},
   "source": [
    "#### parse_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99067db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"\\u00A0\", \" \").replace(\"\\u202F\", \" \").replace(\"\\u200B\", \"\")\n",
    "    return s.strip()\n",
    "\n",
    "def parse_legal_text(text: str):\n",
    "    # Normalize each line and rebuild\n",
    "    clean_lines = [normalize_unicode(line) for line in text.splitlines()]\n",
    "    clean_text = \"\\n\".join(clean_lines)\n",
    "\n",
    "    chapter_pattern = r\"(?i)^\\s*chÆ°Æ¡ng\\s+([IVXLCDM\\d]+)\\b\"\n",
    "    clause_pattern = r\"^\\s*Äiá»u\\s+(\\d+)\\b\"\n",
    "    point_pattern = r\"^\\s*(\\d+)\\.\"\n",
    "    subpoint_pattern = r\"^\\s*([a-z])\\)\"\n",
    "    subsubpoint_pattern = r\"^\\s*([a-z])\\.(\\d+)\\)\"\n",
    "    subsubsubpoint_pattern = r\"^\\s*([a-z])\\.(\\d+)\\.(\\d+)\\)\"\n",
    "\n",
    "    # Detect if document has chapters\n",
    "    has_chapter = any(re.match(chapter_pattern, line) for line in clean_lines)\n",
    "\n",
    "    structure = OrderedDict()\n",
    "    if has_chapter:\n",
    "        structure[\"chapters\"] = OrderedDict()\n",
    "    else:\n",
    "        structure[\"clauses\"] = []\n",
    "\n",
    "    current_chapter = None\n",
    "    current_clause = None\n",
    "    current_point = None\n",
    "    current_subpoint = None\n",
    "    current_subsubpoint = None\n",
    "    current_subsubsubpoint = None\n",
    "\n",
    "    for line in clean_lines:\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Chapter\n",
    "        mch = re.match(chapter_pattern, line)\n",
    "        if mch and has_chapter:\n",
    "            chap_key = f\"chapter {mch.group(1)}\"\n",
    "            structure[\"chapters\"][chap_key] = {\n",
    "                \"title\": line,\n",
    "                \"text\": \"\",\n",
    "                \"clauses\": []\n",
    "            }\n",
    "            current_chapter = chap_key\n",
    "            current_clause = current_point = current_subpoint = current_subsubpoint = current_subsubsubpoint = None\n",
    "            continue\n",
    "\n",
    "        # Clause (Äiá»u)\n",
    "        mcl = re.match(clause_pattern, line)\n",
    "        if mcl:\n",
    "            clause_entry = {\"clause\": mcl.group(1), \"text\": line, \"points\": []}\n",
    "            if has_chapter:\n",
    "                if current_chapter is None:\n",
    "                    current_chapter = \"no_chapter\"\n",
    "                    structure[\"chapters\"].setdefault(current_chapter, {\"title\": \"\", \"text\": \"\", \"clauses\": []})\n",
    "                structure[\"chapters\"][current_chapter][\"clauses\"].append(clause_entry)\n",
    "            else:\n",
    "                structure[\"clauses\"].append(clause_entry)\n",
    "            current_clause = clause_entry\n",
    "            current_point = current_subpoint = current_subsubpoint = current_subsubsubpoint = None\n",
    "            continue\n",
    "\n",
    "        # Point (1.)\n",
    "        mp = re.match(point_pattern, line)\n",
    "        if mp and current_clause is not None:\n",
    "            current_point = {\"point\": mp.group(1), \"text\": line, \"subpoints\": []}\n",
    "            current_clause[\"points\"].append(current_point)\n",
    "            current_subpoint = current_subsubpoint = current_subsubsubpoint = None\n",
    "            continue\n",
    "\n",
    "        # Subpoint (a))\n",
    "        ms = re.match(subpoint_pattern, line)\n",
    "        if ms and current_point is not None:\n",
    "            current_subpoint = {\"subpoint\": ms.group(1), \"text\": line, \"subsubpoints\": []}\n",
    "            current_point[\"subpoints\"].append(current_subpoint)\n",
    "            current_subsubpoint = current_subsubsubpoint = None\n",
    "            continue\n",
    "\n",
    "        # SubSubpoint (a.1))\n",
    "        mss = re.match(subsubpoint_pattern, line)\n",
    "        if mss and current_subpoint is not None:\n",
    "            tag = f\"{mss.group(1)}.{mss.group(2)}\"\n",
    "            current_subsubpoint = {\"subsubpoint\": tag, \"text\": line, \"subsubsubpoints\": []}\n",
    "            current_subpoint[\"subsubpoints\"].append(current_subsubpoint)\n",
    "            current_subsubsubpoint = None\n",
    "            continue\n",
    "\n",
    "        # SubSubSubpoint (a.1.1))\n",
    "        msss = re.match(subsubsubpoint_pattern, line)\n",
    "        if msss and current_subsubpoint is not None:\n",
    "            tag = f\"{msss.group(1)}.{msss.group(2)}.{msss.group(3)}\"\n",
    "            current_subsubsubpoint = {\"subsubsubpoint\": tag, \"text\": line}\n",
    "            current_subsubpoint[\"subsubsubpoints\"].append(current_subsubsubpoint)\n",
    "            continue\n",
    "\n",
    "        # Continuation of content\n",
    "        if current_subsubsubpoint is not None:\n",
    "            current_subsubsubpoint[\"text\"] += \"\\n\" + line\n",
    "        elif current_subsubpoint is not None:\n",
    "            current_subsubpoint[\"text\"] += \"\\n\" + line\n",
    "        elif current_subpoint is not None:\n",
    "            current_subpoint[\"text\"] += \"\\n\" + line\n",
    "        elif current_point is not None:\n",
    "            current_point[\"text\"] += \"\\n\" + line\n",
    "        elif current_clause is not None:\n",
    "            current_clause[\"text\"] += \"\\n\" + line\n",
    "        elif has_chapter and current_chapter is not None:\n",
    "            prev = structure[\"chapters\"][current_chapter][\"text\"]\n",
    "            structure[\"chapters\"][current_chapter][\"text\"] = (prev + \"\\n\" + line) if prev else line\n",
    "\n",
    "    return structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34bde03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_text_from_s3('luat_doanh_nghiep_2025.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae884e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'clauses': [{'clause': '1', 'text': 'Äiá»u 1. Sá»­a Ä‘á»•i, bá»• sung Luáº­t Doanh nghiá»‡p', 'points': [{'point': '1', 'text': '1. Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ khoáº£n cá»§a Äiá»u 4 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) Sá»­a Ä‘á»•i, bá»• sung khoáº£n 5 nhÆ° sau:\\nâ€œ5. Cá»• tá»©c lÃ  khoáº£n lá»£i nhuáº­n sau thuáº¿ Ä‘Æ°á»£c tráº£ cho má»—i cá»• pháº§n báº±ng tiá»n hoáº·c báº±ng tÃ i sáº£n\\nkhÃ¡c.â€;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Sá»­a Ä‘á»•i, bá»• sung khoáº£n 14 nhÆ° sau:\\nâ€œ14. GiÃ¡ thá»‹ trÆ°á»ng cá»§a pháº§n vá»‘n gÃ³p hoáº·c cá»• pháº§n lÃ :', 'subsubpoints': []}, {'subpoint': 'a', 'text': 'a) GiÃ¡ giao dá»‹ch bÃ¬nh quÃ¢n trong vÃ²ng 30 ngÃ y liá»n ká» trÆ°á»›c ngÃ y xÃ¡c Ä‘á»‹nh giÃ¡ hoáº·c giÃ¡ thá»a\\nthuáº­n giá»¯a ngÆ°á»i bÃ¡n vÃ  ngÆ°á»i mua hoáº·c giÃ¡ do má»™t tá»• chá»©c tháº©m Ä‘á»‹nh giÃ¡ xÃ¡c Ä‘á»‹nh Ä‘á»‘i vá»›i cá»•\\nphiáº¿u niÃªm yáº¿t, Ä‘Äƒng kÃ½ giao dá»‹ch trÃªn há»‡ thá»‘ng giao dá»‹ch chá»©ng khoÃ¡n;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) GiÃ¡ giao dá»‹ch trÃªn thá»‹ trÆ°á»ng táº¡i thá»i Ä‘iá»ƒm liá»n ká» trÆ°á»›c Ä‘Ã³ hoáº·c giÃ¡ thá»a thuáº­n giá»¯a ngÆ°á»i bÃ¡n\\nvÃ  ngÆ°á»i mua hoáº·c giÃ¡ do má»™t tá»• chá»©c tháº©m Ä‘á»‹nh giÃ¡ xÃ¡c Ä‘á»‹nh Ä‘á»‘i vá»›i pháº§n vá»‘n gÃ³p hoáº·c cá»•\\npháº§n khÃ´ng thuá»™c Ä‘iá»ƒm a khoáº£n nÃ y.â€;', 'subsubpoints': []}, {'subpoint': 'c', 'text': 'c) Sá»­a Ä‘á»•i, bá»• sung khoáº£n 16 nhÆ° sau:\\nâ€œ16. Giáº¥y tá» phÃ¡p lÃ½ cá»§a cÃ¡ nhÃ¢n lÃ  má»™t trong cÃ¡c loáº¡i giáº¥y tá» sau Ä‘Ã¢y: tháº» CÄƒn cÆ°á»›c, tháº» CÄƒn\\ncÆ°á»›c cÃ´ng dÃ¢n, Há»™ chiáº¿u, giáº¥y tá» chá»©ng thá»±c cÃ¡ nhÃ¢n há»£p phÃ¡p khÃ¡c.â€;', 'subsubpoints': []}, {'subpoint': 'd', 'text': 'd) Bá»• sung khoáº£n 35 vÃ o sau khoáº£n 34 nhÆ° sau:\\nâ€œ35. Chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p cÃ³ tÆ° cÃ¡ch phÃ¡p nhÃ¢n (sau Ä‘Ã¢y gá»i lÃ  chá»§ sá»Ÿ há»¯u\\nhÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p) lÃ  cÃ¡ nhÃ¢n cÃ³ quyá»n sá»Ÿ há»¯u trÃªn thá»±c táº¿ vá»‘n Ä‘iá»u lá»‡ hoáº·c cÃ³ quyá»n\\nchi phá»‘i Ä‘á»‘i vá»›i doanh nghiá»‡p Ä‘Ã³, trá»« trÆ°á»ng há»£p ngÆ°á»i Ä‘áº¡i diá»‡n chá»§ sá»Ÿ há»¯u trá»±c tiáº¿p táº¡i doanh\\nnghiá»‡p do NhÃ  nÆ°á»›c náº¯m giá»¯ 100% vá»‘n Ä‘iá»u lá»‡ vÃ  ngÆ°á»i Ä‘áº¡i diá»‡n pháº§n vá»‘n nhÃ  nÆ°á»›c táº¡i cÃ´ng ty\\ncá»• pháº§n, cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n hai thÃ nh viÃªn trá»Ÿ lÃªn theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t vá» quáº£n\\nlÃ½ vÃ  Ä‘áº§u tÆ° vá»‘n nhÃ  nÆ°á»›c táº¡i doanh nghiá»‡p.â€.', 'subsubpoints': []}]}, {'point': '2', 'text': '2. Bá»• sung khoáº£n 5a vÃ o sau khoáº£n 5 Äiá»u 8 nhÆ° sau:\\nâ€œ5a. Thu tháº­p, cáº­p nháº­t, lÆ°u giá»¯ thÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p; cung cáº¥p\\nthÃ´ng tin cho cÆ¡ quan nhÃ  nÆ°á»›c cÃ³ tháº©m quyá»n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh\\nnghiá»‡p khi Ä‘Æ°á»£c yÃªu cáº§u.â€.', 'subpoints': []}, {'point': '3', 'text': '3. Bá»• sung Ä‘iá»ƒm h vÃ o sau Ä‘iá»ƒm g khoáº£n 1 Äiá»u 11 nhÆ° sau:\\nâ€œh) Danh sÃ¡ch chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³).â€.', 'subpoints': []}, {'point': '4', 'text': '4. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 2 Äiá»u 13 nhÆ° sau:\\nâ€œ2. NgÆ°á»i Ä‘áº¡i diá»‡n theo phÃ¡p luáº­t cá»§a doanh nghiá»‡p chá»‹u trÃ¡ch nhiá»‡m cÃ¡ nhÃ¢n theo quy Ä‘á»‹nh cá»§a\\nphÃ¡p luáº­t Ä‘á»‘i vá»›i thiá»‡t háº¡i cho doanh nghiá»‡p do vi pháº¡m trÃ¡ch nhiá»‡m quy Ä‘á»‹nh táº¡i khoáº£n 1 Äiá»u\\nnÃ y.â€.', 'subpoints': []}, {'point': '5', 'text': '5. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 4 vÃ  khoáº£n 5 Äiá»u 16 nhÆ° sau:\\nâ€œ4. KÃª khai giáº£ máº¡o, kÃª khai khÃ´ng trung thá»±c, kÃª khai khÃ´ng chÃ­nh xÃ¡c ná»™i dung há»“ sÆ¡ Ä‘Äƒng kÃ½\\ndoanh nghiá»‡p vÃ  ná»™i dung há»“ sÆ¡ Ä‘Äƒng kÃ½ thay Ä‘á»•i ná»™i dung Ä‘Äƒng kÃ½ doanh nghiá»‡p.', 'subpoints': []}, {'point': '5', 'text': '5. KÃª khai khá»‘ng vá»‘n Ä‘iá»u lá»‡ thÃ´ng qua hÃ nh vi khÃ´ng gÃ³p Ä‘á»§ sá»‘ vá»‘n Ä‘iá»u lá»‡ nhÆ° Ä‘Ã£ Ä‘Äƒng kÃ½ mÃ \\nkhÃ´ng thá»±c hiá»‡n Ä‘Äƒng kÃ½ Ä‘iá»u chá»‰nh vá»‘n Ä‘iá»u lá»‡ theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t; cá»‘ Ã½ Ä‘á»‹nh giÃ¡ tÃ i sáº£n\\ngÃ³p vá»‘n khÃ´ng Ä‘Ãºng giÃ¡ trá»‹.â€.', 'subpoints': []}, {'point': '6', 'text': '6. Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»ƒm, khoáº£n cá»§a Äiá»u 17 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm b khoáº£n 2 nhÆ° sau:\\nâ€œb) CÃ¡n bá»™, cÃ´ng chá»©c, viÃªn chá»©c theo quy Ä‘á»‹nh cá»§a Luáº­t CÃ¡n bá»™, cÃ´ng chá»©c vÃ  Luáº­t ViÃªn chá»©c,\\ntrá»« trÆ°á»ng há»£p Ä‘Æ°á»£c thá»±c hiá»‡n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t vá» khoa há»c, cÃ´ng nghá»‡, Ä‘á»•i má»›i sÃ¡ng\\ntáº¡o vÃ  chuyá»ƒn Ä‘á»•i sá»‘ quá»‘c gia;â€;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm e khoáº£n 2 nhÆ° sau:\\nâ€œe) NgÆ°á»i Ä‘ang bá»‹ truy cá»©u trÃ¡ch nhiá»‡m hÃ¬nh sá»±, bá»‹ táº¡m giam, Ä‘ang cháº¥p hÃ nh hÃ¬nh pháº¡t tÃ¹, Ä‘ang\\ncháº¥p hÃ nh biá»‡n phÃ¡p xá»­ lÃ½ hÃ nh chÃ­nh táº¡i cÆ¡ sá»Ÿ cai nghiá»‡n báº¯t buá»™c, cÆ¡ sá»Ÿ giÃ¡o dá»¥c báº¯t buá»™c\\nhoáº·c Ä‘ang bá»‹ TÃ²a Ã¡n cáº¥m Ä‘áº£m nhiá»‡m chá»©c vá»¥, cáº¥m hÃ nh nghá» hoáº·c lÃ m cÃ´ng viá»‡c nháº¥t Ä‘á»‹nh; cÃ¡c\\ntrÆ°á»ng há»£p khÃ¡c theo quy Ä‘á»‹nh cá»§a Luáº­t PhÃ¡ sáº£n, Luáº­t PhÃ²ng, chá»‘ng tham nhÅ©ng;â€;', 'subsubpoints': []}, {'subpoint': 'c', 'text': 'c) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm b khoáº£n 3 nhÆ° sau:\\nâ€œb) Äá»‘i tÆ°á»£ng khÃ´ng Ä‘Æ°á»£c gÃ³p vá»‘n vÃ o doanh nghiá»‡p theo quy Ä‘á»‹nh cá»§a Luáº­t CÃ¡n bá»™, cÃ´ng chá»©c,\\nLuáº­t ViÃªn chá»©c vÃ  Luáº­t PhÃ²ng, chá»‘ng tham nhÅ©ng, trá»« trÆ°á»ng há»£p Ä‘Æ°á»£c thá»±c hiá»‡n theo quy Ä‘á»‹nh\\ncá»§a phÃ¡p luáº­t vá» khoa há»c, cÃ´ng nghá»‡, Ä‘á»•i má»›i sÃ¡ng táº¡o vÃ  chuyá»ƒn Ä‘á»•i sá»‘ quá»‘c gia.â€.', 'subsubpoints': []}]}, {'point': '7', 'text': '7. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 3 Äiá»u 20 nhÆ° sau:\\nâ€œ3. Danh sÃ¡ch thÃ nh viÃªn; danh sÃ¡ch chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³).â€.', 'subpoints': []}, {'point': '8', 'text': '8. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 3 Äiá»u 21 nhÆ° sau:\\nâ€œ3. Danh sÃ¡ch thÃ nh viÃªn; danh sÃ¡ch chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³).â€.', 'subpoints': []}, {'point': '9', 'text': '9. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 3 Äiá»u 22 nhÆ° sau:\\nâ€œ3. Danh sÃ¡ch cá»• Ä‘Ã´ng sÃ¡ng láº­p; danh sÃ¡ch cá»• Ä‘Ã´ng lÃ  nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i; danh sÃ¡ch chá»§ sá»Ÿ\\nhá»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³).â€.', 'subpoints': []}, {'point': '10', 'text': '10. Bá»• sung khoáº£n 10 vÃ o sau khoáº£n 9 Äiá»u 23 nhÆ° sau:\\nâ€œ10. ThÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³).â€.', 'subpoints': []}, {'point': '11', 'text': '11. Sá»­a Ä‘á»•i, bá»• sung tÃªn Äiá»u, Ä‘oáº¡n má»Ÿ Ä‘áº§u cá»§a Äiá»u 25 vÃ  bá»• sung khoáº£n 5 vÃ o sau khoáº£n 4', 'subpoints': []}]}, {'clause': '25', 'text': 'Äiá»u 25 nhÆ° sau:\\na) Sá»­a Ä‘á»•i, bá»• sung tÃªn Äiá»u nhÆ° sau:\\nâ€œÄiá»u 25. Danh sÃ¡ch thÃ nh viÃªn cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n, cÃ´ng ty há»£p danh, danh sÃ¡ch\\ncá»• Ä‘Ã´ng sÃ¡ng láº­p vÃ  cá»• Ä‘Ã´ng lÃ  nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i Ä‘á»‘i vá»›i cÃ´ng ty cá»• pháº§n, danh sÃ¡ch\\nchá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p â€;\\nb) Sá»­a Ä‘á»•i, bá»• sung Ä‘oáº¡n má»Ÿ Ä‘áº§u nhÆ° sau:\\nâ€œDanh sÃ¡ch thÃ nh viÃªn cÃ´ng ty trÃ¡ch nhiá»‡m há»¯u háº¡n, cÃ´ng ty há»£p danh, danh sÃ¡ch cá»• Ä‘Ã´ng sÃ¡ng\\nláº­p vÃ  cá»• Ä‘Ã´ng lÃ  nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i Ä‘á»‘i vá»›i cÃ´ng ty cá»• pháº§n, danh sÃ¡ch chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i\\ncá»§a doanh nghiá»‡p pháº£i bao gá»“m cÃ¡c ná»™i dung chá»§ yáº¿u sau Ä‘Ã¢y:â€;\\nc) Bá»• sung khoáº£n 5 vÃ o sau khoáº£n 4 nhÆ° sau:\\nâ€œ5. Danh sÃ¡ch chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p bao gá»“m cÃ¡c ná»™i dung chá»§ yáº¿u sau Ä‘Ã¢y:\\nhá», tÃªn; ngÃ y, thÃ¡ng, nÄƒm sinh; quá»‘c tá»‹ch; dÃ¢n tá»™c; giá»›i tÃ­nh; Ä‘á»‹a chá»‰ liÃªn láº¡c; tá»· lá»‡ sá»Ÿ há»¯u hoáº·c\\nquyá»n chi phá»‘i; thÃ´ng tin vá» giáº¥y tá» phÃ¡p lÃ½ cá»§a cÃ¡ nhÃ¢n chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh\\nnghiá»‡p.â€.', 'points': [{'point': '12', 'text': '12. Sá»­a Ä‘á»•i, bá»• sung, bÃ£i bá» má»™t sá»‘ khoáº£n cá»§a Äiá»u 26 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) BÃ£i bá» khoáº£n 3 vÃ  khoáº£n 4;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Sá»­a Ä‘á»•i, bá»• sung khoáº£n 6 nhÆ° sau:\\nâ€œ6. ChÃ­nh phá»§ quy Ä‘á»‹nh vá» há»“ sÆ¡, trÃ¬nh tá»±, thá»§ tá»¥c, liÃªn thÃ´ng trong Ä‘Äƒng kÃ½ doanh nghiá»‡p, viá»‡c\\nÄ‘Äƒng kÃ½ doanh nghiá»‡p qua máº¡ng thÃ´ng tin Ä‘iá»‡n tá»­.â€.', 'subsubpoints': []}]}, {'point': '13', 'text': '13. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 1 Äiá»u 31 nhÆ° sau:\\nâ€œ1. Doanh nghiá»‡p pháº£i thÃ´ng bÃ¡o vá»›i CÆ¡ quan Ä‘Äƒng kÃ½ kinh doanh khi cÃ³ thay Ä‘á»•i má»™t trong cÃ¡c\\nná»™i dung sau Ä‘Ã¢y:', 'subpoints': [{'subpoint': 'a', 'text': 'a) NgÃ nh, nghá» kinh doanh;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Cá»• Ä‘Ã´ng sÃ¡ng láº­p vÃ  cá»• Ä‘Ã´ng lÃ  nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i Ä‘á»‘i vá»›i cÃ´ng ty cá»• pháº§n, trá»« trÆ°á»ng há»£p\\nÄ‘á»‘i vá»›i cÃ´ng ty niÃªm yáº¿t vÃ  cÃ´ng ty Ä‘Äƒng kÃ½ giao dá»‹ch chá»©ng khoÃ¡n;', 'subsubpoints': []}, {'subpoint': 'c', 'text': 'c) ThÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p, trá»« trÆ°á»ng há»£p Ä‘á»‘i vá»›i cÃ´ng ty niÃªm yáº¿t\\nvÃ  cÃ´ng ty Ä‘Äƒng kÃ½ giao dá»‹ch chá»©ng khoÃ¡n;', 'subsubpoints': []}, {'subpoint': 'd', 'text': 'd) Ná»™i dung khÃ¡c trong há»“ sÆ¡ Ä‘Äƒng kÃ½ doanh nghiá»‡p.â€.', 'subsubpoints': []}]}, {'point': '14', 'text': '14. Bá»• sung khoáº£n 1a vÃ o sau khoáº£n 1 Äiá»u 33 nhÆ° sau:\\nâ€œ1a. CÆ¡ quan nhÃ  nÆ°á»›c cÃ³ tháº©m quyá»n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t cÃ³ quyá»n Ä‘á» nghá»‹ CÆ¡ quan\\nquáº£n lÃ½ nhÃ  nÆ°á»›c vá» Ä‘Äƒng kÃ½ kinh doanh cung cáº¥p thÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh\\nnghiá»‡p Ä‘Æ°á»£c lÆ°u giá»¯ trÃªn Há»‡ thá»‘ng thÃ´ng tin quá»‘c gia vá» Ä‘Äƒng kÃ½ doanh nghiá»‡p Ä‘á»ƒ phá»¥c vá»¥ cÃ´ng\\ntÃ¡c vá» phÃ²ng, chá»‘ng rá»­a tiá»n vÃ  khÃ´ng pháº£i tráº£ phÃ­.â€.', 'subpoints': []}, {'point': '15', 'text': '15. Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm a khoáº£n 1 Äiá»u 52 nhÆ° sau:\\nâ€œa) ChÃ o bÃ¡n pháº§n vá»‘n gÃ³p Ä‘Ã³ cho thÃ nh viÃªn cÃ²n láº¡i theo tá»· lá»‡ tÆ°Æ¡ng á»©ng vá»›i pháº§n vá»‘n gÃ³p cá»§a\\nthÃ nh viÃªn cÃ²n láº¡i trong cÃ´ng ty vá»›i cÃ¹ng Ä‘iá»u kiá»‡n chÃ o bÃ¡n;â€.', 'subpoints': []}, {'point': '16', 'text': '16. Bá»• sung khoáº£n 9 vÃ o sau khoáº£n 8 Äiá»u 57 nhÆ° sau:\\nâ€œ9. Ná»™i dung liÃªn quan Ä‘áº¿n trÃ¬nh tá»±, thá»§ tá»¥c má»i há»p, triá»‡u táº­p há»p Há»™i Ä‘á»“ng thÃ nh viÃªn trong\\ntrÆ°á»ng há»£p quy Ä‘á»‹nh táº¡i khoáº£n 4 Äiá»u 56 thá»±c hiá»‡n tÆ°Æ¡ng á»©ng theo cÃ¡c quy Ä‘á»‹nh táº¡i cÃ¡c khoáº£n 2,\\n3, 4, 5 vÃ  6 Äiá»u nÃ y. Chi phÃ­ há»£p lÃ½ cho viá»‡c triá»‡u táº­p vÃ  tiáº¿n hÃ nh há»p Há»™i Ä‘á»“ng thÃ nh viÃªn sáº½\\nÄ‘Æ°á»£c cÃ´ng ty hoÃ n láº¡i.â€.', 'subpoints': []}, {'point': '17', 'text': '17. Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»ƒm cá»§a khoáº£n 5 Äiá»u 112 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm a nhÆ° sau:\\nâ€œa) Theo quyáº¿t Ä‘á»‹nh cá»§a Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng, cÃ´ng ty hoÃ n tráº£ má»™t pháº§n vá»‘n gÃ³p cho cá»• Ä‘Ã´ng\\ntheo tá»· lá»‡ sá»Ÿ há»¯u cá»• pháº§n cá»§a há» trong cÃ´ng ty náº¿u cÃ´ng ty Ä‘Ã£ hoáº¡t Ä‘á»™ng kinh doanh tá»« 02 nÄƒm\\ntrá»Ÿ lÃªn ká»ƒ tá»« ngÃ y Ä‘Äƒng kÃ½ thÃ nh láº­p doanh nghiá»‡p khÃ´ng ká»ƒ thá»i gian Ä‘Äƒng kÃ½ táº¡m ngá»«ng kinh\\ndoanh vÃ  báº£o Ä‘áº£m thanh toÃ¡n Ä‘á»§ cÃ¡c khoáº£n ná»£ vÃ  nghÄ©a vá»¥ tÃ i sáº£n khÃ¡c sau khi Ä‘Ã£ hoÃ n tráº£ cho\\ncá»• Ä‘Ã´ng;â€;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Bá»• sung Ä‘iá»ƒm d vÃ o sau Ä‘iá»ƒm c nhÆ° sau:\\nâ€œd) CÃ´ng ty hoÃ n láº¡i vá»‘n gÃ³p theo yÃªu cáº§u, Ä‘iá»u kiá»‡n Ä‘Æ°á»£c ghi táº¡i cá»• phiáº¿u cho cá»• Ä‘Ã´ng sá»Ÿ há»¯u\\ncá»• pháº§n cÃ³ quyá»n Æ°u Ä‘Ã£i hoÃ n láº¡i theo quy Ä‘á»‹nh cá»§a Luáº­t nÃ y vÃ  Äiá»u lá»‡ cÃ´ng ty.â€.', 'subsubpoints': []}]}, {'point': '18', 'text': '18. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 4 Äiá»u 115 nhÆ° sau:\\nâ€œ4. YÃªu cáº§u triá»‡u táº­p há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng quy Ä‘á»‹nh táº¡i khoáº£n 3 Äiá»u nÃ y pháº£i báº±ng vÄƒn báº£n\\nvÃ  pháº£i bao gá»“m cÃ¡c ná»™i dung sau Ä‘Ã¢y: há», tÃªn, Ä‘á»‹a chá»‰ liÃªn láº¡c, quá»‘c tá»‹ch, sá»‘ giáº¥y tá» phÃ¡p lÃ½ cá»§a\\ncÃ¡ nhÃ¢n Ä‘á»‘i vá»›i cá»• Ä‘Ã´ng lÃ  cÃ¡ nhÃ¢n; tÃªn, mÃ£ sá»‘ doanh nghiá»‡p hoáº·c sá»‘ giáº¥y tá» phÃ¡p lÃ½ cá»§a tá»• chá»©c,\\nÄ‘á»‹a chá»‰ trá»¥ sá»Ÿ chÃ­nh Ä‘á»‘i vá»›i cá»• Ä‘Ã´ng lÃ  tá»• chá»©c; sá»‘ lÆ°á»£ng cá»• pháº§n vÃ  thá»i Ä‘iá»ƒm Ä‘Äƒng kÃ½ cá»• pháº§n\\ncá»§a tá»«ng cá»• Ä‘Ã´ng, tá»•ng sá»‘ cá»• pháº§n cá»§a cáº£ nhÃ³m cá»• Ä‘Ã´ng vÃ  tá»· lá»‡ sá»Ÿ há»¯u trong tá»•ng sá»‘ cá»• pháº§n\\ncá»§a cÃ´ng ty, cÄƒn cá»© vÃ  lÃ½ do yÃªu cáº§u triá»‡u táº­p há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng. KÃ¨m theo yÃªu cáº§u triá»‡u\\ntáº­p há»p pháº£i cÃ³ cÃ¡c tÃ i liá»‡u, chá»©ng cá»© vá» cÃ¡c vi pháº¡m cá»§a Há»™i Ä‘á»“ng quáº£n trá»‹, má»©c Ä‘á»™ vi pháº¡m\\nhoáº·c vá» quyáº¿t Ä‘á»‹nh vÆ°á»£t quÃ¡ tháº©m quyá»n. Cá»• Ä‘Ã´ng hoáº·c nhÃ³m cá»• Ä‘Ã´ng chá»‹u hoÃ n toÃ n trÃ¡ch\\nnhiá»‡m trÆ°á»›c phÃ¡p luáº­t vá» tÃ­nh chÃ­nh xÃ¡c, trung thá»±c cá»§a cÃ¡c tÃ i liá»‡u, chá»©ng cá»© cung cáº¥p cho cÆ¡\\nquan cÃ³ tháº©m quyá»n khi yÃªu cáº§u triá»‡u táº­p há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng.â€.', 'subpoints': []}, {'point': '19', 'text': '19. Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»ƒm, khoáº£n cá»§a Äiá»u 128 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm b khoáº£n 2 nhÆ° sau:\\nâ€œb) NhÃ  Ä‘áº§u tÆ° chá»©ng khoÃ¡n chuyÃªn nghiá»‡p tham gia mua, giao dá»‹ch, chuyá»ƒn nhÆ°á»£ng trÃ¡i phiáº¿u\\nriÃªng láº» thá»±c hiá»‡n theo quy Ä‘á»‹nh phÃ¡p luáº­t vá» chá»©ng khoÃ¡n.â€;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Bá»• sung Ä‘iá»ƒm c1 vÃ o sau Ä‘iá»ƒm c khoáº£n 3 nhÆ° sau:\\nâ€œc1) CÃ³ ná»£ pháº£i tráº£ (bao gá»“m giÃ¡ trá»‹ trÃ¡i phiáº¿u dá»± kiáº¿n phÃ¡t hÃ nh) khÃ´ng vÆ°á»£t quÃ¡ 05 láº§n vá»‘n chá»§\\nsá»Ÿ há»¯u cá»§a tá»• chá»©c phÃ¡t hÃ nh theo bÃ¡o cÃ¡o tÃ i chÃ­nh nÄƒm liá»n ká» trÆ°á»›c nÄƒm phÃ¡t hÃ nh Ä‘Æ°á»£c kiá»ƒm\\ntoÃ¡n; trá»« tá»• chá»©c phÃ¡t hÃ nh lÃ  doanh nghiá»‡p nhÃ  nÆ°á»›c, doanh nghiá»‡p phÃ¡t hÃ nh trÃ¡i phiáº¿u Ä‘á»ƒ thá»±c\\nhiá»‡n dá»± Ã¡n báº¥t Ä‘á»™ng sáº£n, tá»• chá»©c tÃ­n dá»¥ng, doanh nghiá»‡p báº£o hiá»ƒm, doanh nghiá»‡p tÃ¡i báº£o hiá»ƒm,\\ndoanh nghiá»‡p mÃ´i giá»›i báº£o hiá»ƒm, cÃ´ng ty chá»©ng khoÃ¡n, cÃ´ng ty quáº£n lÃ½ quá»¹ Ä‘áº§u tÆ° chá»©ng khoÃ¡n\\nthá»±c hiá»‡n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t cÃ³ liÃªn quan;â€.', 'subsubpoints': []}]}, {'point': '20', 'text': '20. Bá»• sung khoáº£n 4a vÃ o sau khoáº£n 4 Äiá»u 140 nhÆ° sau:\\nâ€œ4a. Äá»‘i vá»›i cÃ´ng ty cÃ³ cÆ¡ cáº¥u tá»• chá»©c quáº£n lÃ½ theo quy Ä‘á»‹nh táº¡i Ä‘iá»ƒm b khoáº£n 1 Äiá»u 137,\\ntrÆ°á»ng há»£p Há»™i Ä‘á»“ng quáº£n trá»‹ khÃ´ng triá»‡u táº­p há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng theo quy Ä‘á»‹nh táº¡i khoáº£n\\n2 Äiá»u nÃ y thÃ¬ trong thá»i háº¡n 30 ngÃ y tiáº¿p theo, cá»• Ä‘Ã´ng hoáº·c nhÃ³m cá»• Ä‘Ã´ng theo quy Ä‘á»‹nh táº¡i\\nkhoáº£n 2 Äiá»u 115 cá»§a Luáº­t nÃ y cÃ³ quyá»n Ä‘áº¡i diá»‡n cÃ´ng ty triá»‡u táº­p há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng\\ntheo quy Ä‘á»‹nh cá»§a Luáº­t nÃ y. Chi phÃ­ há»£p lÃ½ cho viá»‡c triá»‡u táº­p vÃ  tiáº¿n hÃ nh há»p Äáº¡i há»™i Ä‘á»“ng cá»•\\nÄ‘Ã´ng sáº½ Ä‘Æ°á»£c cÃ´ng ty hoÃ n láº¡i.â€.', 'subpoints': []}, {'point': '21', 'text': '21. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 1 Äiá»u 141 nhÆ° sau:\\nâ€œ1. Danh sÃ¡ch cá»• Ä‘Ã´ng cÃ³ quyá»n dá»± há»p Äáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng Ä‘Æ°á»£c láº­p dá»±a trÃªn sá»• Ä‘Äƒng kÃ½ cá»•\\nÄ‘Ã´ng, sá»• Ä‘Äƒng kÃ½ ngÆ°á»i sá»Ÿ há»¯u chá»©ng khoÃ¡n cá»§a cÃ´ng ty. Danh sÃ¡ch cá»• Ä‘Ã´ng cÃ³ quyá»n dá»± há»p\\nÄáº¡i há»™i Ä‘á»“ng cá»• Ä‘Ã´ng Ä‘Æ°á»£c láº­p khÃ´ng quÃ¡ 10 ngÃ y trÆ°á»›c ngÃ y gá»­i giáº¥y má»i há»p Äáº¡i há»™i Ä‘á»“ng cá»•\\nÄ‘Ã´ng náº¿u Äiá»u lá»‡ cÃ´ng ty khÃ´ng quy Ä‘á»‹nh thá»i háº¡n ngáº¯n hÆ¡n.â€.', 'subpoints': []}, {'point': '22', 'text': '22. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 3 Äiá»u 176 nhÆ° sau:\\nâ€œ3. CÃ´ng ty cá»• pháº§n, trá»« cÃ´ng ty niÃªm yáº¿t vÃ  cÃ´ng ty Ä‘Äƒng kÃ½ giao dá»‹ch chá»©ng khoÃ¡n, pháº£i thÃ´ng\\nbÃ¡o cho CÆ¡ quan Ä‘Äƒng kÃ½ kinh doanh cháº­m nháº¥t lÃ  03 ngÃ y lÃ m viá»‡c sau khi cÃ³ thÃ´ng tin hoáº·c cÃ³\\nthay Ä‘á»•i cÃ¡c thÃ´ng tin vá» há», tÃªn, quá»‘c tá»‹ch, sá»‘ há»™ chiáº¿u, Ä‘á»‹a chá»‰ liÃªn láº¡c, sá»‘ cá»• pháº§n vÃ  loáº¡i cá»•\\npháº§n cá»§a cá»• Ä‘Ã´ng lÃ  cÃ¡ nhÃ¢n nÆ°á»›c ngoÃ i; tÃªn, mÃ£ sá»‘ doanh nghiá»‡p, Ä‘á»‹a chá»‰ trá»¥ sá»Ÿ chÃ­nh, sá»‘ cá»•\\npháº§n vÃ  loáº¡i cá»• pháº§n cá»§a cá»• Ä‘Ã´ng lÃ  tá»• chá»©c nÆ°á»›c ngoÃ i vÃ  há», tÃªn, quá»‘c tá»‹ch, sá»‘ há»™ chiáº¿u, Ä‘á»‹a chá»‰\\nliÃªn láº¡c ngÆ°á»i Ä‘áº¡i diá»‡n theo á»§y quyá»n cá»§a cá»• Ä‘Ã´ng lÃ  tá»• chá»©c nÆ°á»›c ngoÃ i.â€.', 'subpoints': []}, {'point': '23', 'text': '23. Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm c khoáº£n 1 Äiá»u 207 nhÆ° sau:\\nâ€œc) CÃ´ng ty khÃ´ng cÃ²n Ä‘á»§ sá»‘ lÆ°á»£ng thÃ nh viÃªn, cá»• Ä‘Ã´ng tá»‘i thiá»ƒu theo quy Ä‘á»‹nh cá»§a Luáº­t nÃ y\\ntrong thá»i háº¡n 06 thÃ¡ng liÃªn tá»¥c mÃ  khÃ´ng lÃ m thá»§ tá»¥c chuyá»ƒn Ä‘á»•i loáº¡i hÃ¬nh doanh nghiá»‡p;â€.', 'subpoints': []}, {'point': '24', 'text': '24. Sá»­a Ä‘á»•i, bá»• sung khoáº£n 1 Äiá»u 213 nhÆ° sau:\\nâ€œ1. Chi nhÃ¡nh, vÄƒn phÃ²ng Ä‘áº¡i diá»‡n, Ä‘á»‹a Ä‘iá»ƒm kinh doanh cá»§a doanh nghiá»‡p Ä‘Æ°á»£c cháº¥m dá»©t hoáº¡t\\nÄ‘á»™ng theo quyáº¿t Ä‘á»‹nh cá»§a chÃ­nh doanh nghiá»‡p Ä‘Ã³ hoáº·c theo quyáº¿t Ä‘á»‹nh thu há»“i Giáº¥y chá»©ng nháº­n\\nÄ‘Äƒng kÃ½ doanh nghiá»‡p, hoáº¡t Ä‘á»™ng chi nhÃ¡nh, vÄƒn phÃ²ng Ä‘áº¡i diá»‡n, Ä‘á»‹a Ä‘iá»ƒm kinh doanh cá»§a cÆ¡\\nquan nhÃ  nÆ°á»›c cÃ³ tháº©m quyá»n.â€.', 'subpoints': []}, {'point': '25', 'text': '25. Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»ƒm, khoáº£n cá»§a Äiá»u 215 nhÆ° sau:', 'subpoints': [{'subpoint': 'a', 'text': 'a) Sá»­a Ä‘á»•i, bá»• sung khoáº£n 3 nhÆ° sau:\\nâ€œ3. á»¦y ban nhÃ¢n dÃ¢n cáº¥p tá»‰nh thá»±c hiá»‡n quáº£n lÃ½ nhÃ  nÆ°á»›c Ä‘á»‘i vá»›i doanh nghiá»‡p trong pháº¡m vi Ä‘á»‹a\\nphÆ°Æ¡ng, cÃ³ trÃ¡ch nhiá»‡m tá»• chá»©c CÆ¡ quan Ä‘Äƒng kÃ½ kinh doanh, ban hÃ nh quy trÃ¬nh kiá»ƒm tra ná»™i\\ndung vá» Ä‘Äƒng kÃ½ kinh doanh trÃªn Ä‘á»‹a bÃ n báº£o Ä‘áº£m cÃ´ng khai, minh báº¡ch.â€;', 'subsubpoints': []}, {'subpoint': 'b', 'text': 'b) Sá»­a Ä‘á»•i, bá»• sung Ä‘iá»ƒm c khoáº£n 4 nhÆ° sau:\\nâ€œc) Phá»‘i há»£p, chia sáº» thÃ´ng tin vá» tÃ¬nh hÃ¬nh hoáº¡t Ä‘á»™ng cá»§a doanh nghiá»‡p, tÃ¬nh tráº¡ng phÃ¡p lÃ½ cá»§a\\ndoanh nghiá»‡p Ä‘á»ƒ nÃ¢ng cao hiá»‡u lá»±c quáº£n lÃ½ nhÃ  nÆ°á»›c.â€;', 'subsubpoints': []}, {'subpoint': 'c', 'text': 'c) Bá»• sung khoáº£n 4a vÃ o sau khoáº£n 4 nhÆ° sau:\\nâ€œ4a. TrÆ°á»ng há»£p doanh nghiá»‡p Ä‘Æ°á»£c thÃ nh láº­p vÃ  hoáº¡t Ä‘á»™ng theo luáº­t quáº£n lÃ½ ngÃ nh, lÄ©nh vá»±c thÃ¬\\ncÆ¡ quan cáº¥p Ä‘Äƒng kÃ½ cÃ³ trÃ¡ch nhiá»‡m tÃ­ch há»£p, chia sáº», cáº­p nháº­t thÃ´ng tin vá» Ä‘Äƒng kÃ½, thÃ nh láº­p\\ndoanh nghiá»‡p vá»›i Há»‡ thá»‘ng thÃ´ng tin quá»‘c gia vá» Ä‘Äƒng kÃ½ doanh nghiá»‡p.â€.', 'subsubpoints': []}]}, {'point': '26', 'text': '26. Bá»• sung Ä‘iá»ƒm h vÃ o sau Ä‘iá»ƒm g khoáº£n 1 Äiá»u 216 nhÆ° sau:\\nâ€œh) LÆ°u giá»¯ thÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p Ã­t nháº¥t 05 nÄƒm ká»ƒ tá»« ngÃ y\\ndoanh nghiá»‡p giáº£i thá»ƒ, phÃ¡ sáº£n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t.â€.', 'subpoints': []}, {'point': '27', 'text': '27. Bá»• sung khoáº£n 6 vÃ o sau khoáº£n 5 Äiá»u 217 nhÆ° sau:\\nâ€œ6. ChÃ­nh phá»§ quy Ä‘á»‹nh chi tiáº¿t tiÃªu chÃ­ xÃ¡c Ä‘á»‹nh, chá»§ thá»ƒ kÃª khai vÃ  viá»‡c kÃª khai thÃ´ng tin vá» chá»§\\nsá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p, thÃ´ng tin Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh\\nnghiá»‡p, cung cáº¥p, lÆ°u giá»¯, chia sáº» thÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p.â€.', 'subpoints': []}, {'point': '28', 'text': '28. Thay tháº¿ tá»« â€œsÃ¡ch nhiá»…uâ€ báº±ng tá»« â€œnhÅ©ng nhiá»…uâ€ táº¡i khoáº£n 1 Äiá»u 16.', 'subpoints': []}]}, {'clause': '2', 'text': 'Äiá»u 2. Hiá»‡u lá»±c thi hÃ nh\\nLuáº­t nÃ y cÃ³ hiá»‡u lá»±c thi hÃ nh tá»« ngÃ y 01 thÃ¡ng 7 nÄƒm 2025.', 'points': []}, {'clause': '3', 'text': 'Äiá»u 3. Äiá»u khoáº£n chuyá»ƒn tiáº¿p', 'points': [{'point': '1', 'text': '1. Äá»‘i vá»›i doanh nghiá»‡p Ä‘Æ°á»£c Ä‘Äƒng kÃ½ thÃ nh láº­p trÆ°á»›c thá»i Ä‘iá»ƒm Luáº­t nÃ y cÃ³ hiá»‡u lá»±c thi hÃ nh\\nthÃ¬ viá»‡c bá»• sung thÃ´ng tin vá» chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³), thÃ´ng tin Ä‘á»ƒ xÃ¡c\\nÄ‘á»‹nh chá»§ sá»Ÿ há»¯u hÆ°á»Ÿng lá»£i cá»§a doanh nghiá»‡p (náº¿u cÃ³) Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»“ng thá»i táº¡i thá»i Ä‘iá»ƒm\\ndoanh nghiá»‡p thá»±c hiá»‡n thá»§ tá»¥c Ä‘Äƒng kÃ½ thay Ä‘á»•i ná»™i dung Ä‘Äƒng kÃ½ doanh nghiá»‡p, thÃ´ng bÃ¡o thay\\nÄ‘á»•i ná»™i dung Ä‘Äƒng kÃ½ doanh nghiá»‡p gáº§n nháº¥t, trá»« trÆ°á»ng há»£p doanh nghiá»‡p cÃ³ yÃªu cáº§u bá»• sung\\nthÃ´ng tin sá»›m hÆ¡n.', 'subpoints': []}, {'point': '2', 'text': '2. Äá»‘i vá»›i cÃ¡c Ä‘á»£t chÃ o bÃ¡n trÃ¡i phiáº¿u doanh nghiá»‡p riÃªng láº» Ä‘Ã£ gá»­i ná»™i dung cÃ´ng bá»‘ thÃ´ng tin\\ntrÆ°á»›c Ä‘á»£t chÃ o bÃ¡n cho Sá»Ÿ giao dá»‹ch chá»©ng khoÃ¡n trÆ°á»›c ngÃ y Luáº­t nÃ y cÃ³ hiá»‡u lá»±c thi hÃ nh thÃ¬\\ntiáº¿p tá»¥c thá»±c hiá»‡n theo quy Ä‘á»‹nh cá»§a Luáº­t Doanh nghiá»‡p sá»‘ 59/2020/QH14 Ä‘Ã£ Ä‘Æ°á»£c sá»­a Ä‘á»•i, bá»•\\nsung má»™t sá»‘ Ä‘iá»u theo Luáº­t sá»‘ 03/2022/QH15.\\nLuáº­t nÃ y Ä‘Æ°á»£c Quá»‘c há»™i nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam khÃ³a XV, Ká»³ há»p thá»© 9\\nthÃ´ng qua ngÃ y 17 thÃ¡ng 6 nÄƒm 2025.\\nCHá»¦ Tá»ŠCH QUá»C Há»˜I\\nTráº§n Thanh Máº«n', 'subpoints': []}]}]})\n"
     ]
    }
   ],
   "source": [
    "print(parse_legal_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1fd41",
   "metadata": {},
   "source": [
    "#### saving_neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecc2bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "QUá»C Há»˜I  \n",
    "-------  Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM  \n",
    "Äá»™c láº­p - Tá»± do - Háº¡nh phÃºc  \n",
    "---------------  \n",
    "Luáº­t sá»‘: 23/2026/QH  HÃ  Ná»™i, ngÃ y 20 thÃ¡ng 1 nÄƒm 2026\n",
    "  \n",
    "LUáº¬T  \n",
    "THUáº¾ Thu nháº­p Máº«u\n",
    "CÄƒn cá»© Hiáº¿n phÃ¡p nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam; \n",
    "CÄƒn cá»© Luáº­t thuáº¿ Thu Nháº­p Doanh Nghiá»‡p sá»‘ 67/2025/QH15;\n",
    "Quá»‘c há»™i ban hÃ nh Luáº­t Thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh  \n",
    "Luáº­t nÃ y quy Ä‘á»‹nh vá» ngÆ°á»i ná»™p thuáº¿, thu nháº­p chá»‹u thuáº¿, thu nháº­p Ä‘Æ°á»£c miá»…n thuáº¿, cÄƒn cá»© tÃ­nh \n",
    "thuáº¿, phÆ°Æ¡ng phÃ¡p tÃ­nh thuáº¿ vÃ  Æ°u Ä‘Ã£i thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "1.CÃ¡c quy Ä‘á»‹nh Ä‘Æ°á»£c liá»‡t kÃª táº¡i Äiá»u nÃ y Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ báº±ng cÃ¡c Ä‘iá»u má»›i.\n",
    "\n",
    "a) CÃ¡c quy Ä‘á»‹nh táº¡i khoáº£n 1 Äiá»u 1 Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ bÃªn dÆ°á»›i.\n",
    "\n",
    "a.1) CÃ¡c quy Ä‘á»‹nh táº¡i Äiá»ƒm a) khoáº£n 1 Ä‘iá»u nÃ y Ä‘Æ°á»£c bá»• sung.\n",
    "\n",
    "a.1.1) CÃ¡c quy Ä‘á»‹nh táº¡i Ä‘iá»ƒm a.1) khoáº£n 1 Äiá»u nÃ y Ä‘Æ°á»£c sá»­a Ä‘á»•i.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f54df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_neo4j(text, namespace=\"Test\"):\n",
    "   # Extract metadata\n",
    "    df_meta = ner.extract_document_metadata(text)\n",
    "    meta_row = df_meta.iloc[0]\n",
    "    df_relation = final_re.final_relation(text)\n",
    "    \n",
    "    self_check = {'luáº­t': 'doc1', 'Ä‘á»‹nh': 'doc2', 'tÆ°': 'doc3', 'quyáº¿t': 'doc4', 'chÆ°Æ¡ng': 'chapter', 'Ä‘iá»u': 'clause', 'má»¥c': 'point'}\n",
    "\n",
    "    metadata = {\n",
    "        \"law_id\": meta_row[\"document_id\"],\n",
    "        \"title\": meta_row[\"title\"],\n",
    "        \"issuer\": meta_row[\"issuer\"],\n",
    "        \"issue_date\": meta_row[\"issue_date\"],\n",
    "        \"location\": meta_row[\"location\"],\n",
    "        \"issuer_department\": meta_row[\"issuer_department\"],\n",
    "        \"document_type\": meta_row[\"document_type\"],\n",
    "        \"amendment_history\": \"None\",\n",
    "        \"jurisdiction_scope\": \"National\",\n",
    "    }\n",
    "\n",
    "    # Document type\n",
    "    doc_type_label = metadata[\"document_type\"].replace(\" \", \"_\").capitalize()\n",
    "    # Namespace\n",
    "    ns_label = re.sub(r\"\\W+\", \"_\", namespace)\n",
    "\n",
    "    # central document node\n",
    "    dml_ddl_neo4j(\n",
    "        f\"\"\"\n",
    "        MERGE (l:`{doc_type_label}`:`{ns_label}` {{id: $law_id}})\n",
    "        SET l += $meta\n",
    "        \"\"\",\n",
    "        law_id=metadata[\"law_id\"],\n",
    "        meta=metadata,\n",
    "    )\n",
    "    \n",
    "    #Connect reference node\n",
    "    for i in range(len(df_relation)):\n",
    "        \n",
    "        doc_type = df_relation.iloc[i,7].replace(\" \", \"_\").capitalize()\n",
    "        \n",
    "        if (len(df_relation.iloc[i,6].split('/')) > 1) and (df_relation.iloc[i,7]):\n",
    "            \n",
    "            dml_ddl_neo4j(\n",
    "            f\"\"\"\n",
    "            MERGE (l:`{doc_type}`:`{ns_label}` {{id: $law_id}})\n",
    "            WITH l\n",
    "            MATCH (r: `{doc_type_label}`:`{ns_label}` {{id: $law_id2}})\n",
    "            MERGE (r)-[:`{df_relation.iloc[i,2]}`]->(l)\n",
    "            \"\"\",\n",
    "            law_id=df_relation.iloc[i,6],\n",
    "            law_id2=metadata['law_id']\n",
    "        )\n",
    "            if (df_relation.iloc[i,4]):  \n",
    "                dml_ddl_neo4j(\n",
    "                    f\"\"\"\n",
    "                    MATCH (l:`{doc_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                    SET l.issue_date = $issue_date\n",
    "                    \"\"\",\n",
    "                    law_id=df_relation.iloc[i,6],\n",
    "                    issue_date=str(df_relation.iloc[i,4])\n",
    "                )\n",
    "                \n",
    "        #if id not available, use date\n",
    "        if df_relation.iloc[i,4]:\n",
    "            dml_ddl_neo4j(\n",
    "            f\"\"\"\n",
    "            MERGE (l:`{doc_type}`:`{ns_label}` {{issue_date: $issue_date}})\n",
    "            WITH l\n",
    "            MATCH (r: `{doc_type_label}`:`{ns_label}` {{id: $law_id2}})\n",
    "            MERGE (r)-[:`{df_relation.iloc[i,2]}`]->(l)\n",
    "            \"\"\",\n",
    "            issue_date=str(df_relation.iloc[i,4]),\n",
    "            law_id2=metadata['law_id']\n",
    "        )\n",
    "            \n",
    "    # Parse structure\n",
    "    parsed = parse_legal_text(text)\n",
    "\n",
    "    # Extract text\n",
    "    def get_text(node, *keys):\n",
    "        for k in keys:\n",
    "            if isinstance(node, dict) and k in node and node[k]:\n",
    "                return node[k]\n",
    "        if isinstance(node, str):\n",
    "            return node\n",
    "        return \"\"\n",
    "\n",
    "    # If HAS chapters \n",
    "    if \"chapters\" in parsed:\n",
    "        for chapter_key, chapter_obj in parsed[\"chapters\"].items():\n",
    "            chapter_id = f\"{metadata['law_id']}_{chapter_key.replace(' ', '_')}\"\n",
    "            chapter_title = get_text(chapter_obj, \"title\")\n",
    "            chapter_text = get_text(chapter_obj, \"text\")\n",
    "            re_text = None\n",
    "            re_temp = None\n",
    "            relation = None\n",
    "            second_entity = []\n",
    "                          \n",
    "            dml_ddl_neo4j(\n",
    "                f\"\"\"\n",
    "                MERGE (ch:Chapter:{ns_label} {{id: $id}})\n",
    "                SET ch.title = $title, ch.text = $text\n",
    "                WITH ch\n",
    "                MATCH (l:{ns_label} {{law_id: $law_id}})\n",
    "                MERGE (l)-[:HAS_CHAPTER]->(ch)\n",
    "                \"\"\",\n",
    "                id=chapter_id,\n",
    "                title=chapter_title,\n",
    "                text=chapter_text,\n",
    "                law_id=metadata[\"law_id\"],\n",
    "            )\n",
    "                    \n",
    "            #Extract relation\n",
    "            chapter_text = re.sub(r'^(ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*', '', chapter_text, flags=re.IGNORECASE)\n",
    "            re_text = sent_tokenize(chapter_text)\n",
    "            re_text[0] = re_text[0].split(chapter_id.split('_')[-1], 1)[1].strip()\n",
    "            for sentence in re_text:\n",
    "                root = None\n",
    "                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                if len(re_temp['document_id']) > 0:\n",
    "                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                if 'nÃ y' in sentence.split():\n",
    "                    words = sentence.split()\n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    for i, token in enumerate(words):\n",
    "                        if token == \"nÃ y\" and i > 0:\n",
    "                            prev_word = words[i-1]\n",
    "                            if prev_word.lower() in self_check.keys():\n",
    "                                text_type = self_check[prev_word.lower()]\n",
    "                                match text_type:\n",
    "                                    case \"doc1\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Luáº­t\"\n",
    "                                    case \"doc2\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                    case \"doc3\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                    case \"doc4\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                    case \"chapter\":\n",
    "                                        root = chapter_id\n",
    "                                    case \"clause\":\n",
    "                                        root = clause_id\n",
    "                                    case \"point\":\n",
    "                                        root = point_id\n",
    "                                if root is not None:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                else:\n",
    "                                    second_entity = []   \n",
    "                                         \n",
    "                if not second_entity or not relation:\n",
    "                    continue               \n",
    "                for entity in second_entity or []:\n",
    "                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                    label = list(entity.keys())[0]       \n",
    "                    ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                        WITH l\n",
    "                        MATCH (r:Chapter:`{ns_label}` {{id: $law_id2}})\n",
    "                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                        \"\"\",\n",
    "                        law_id=label,\n",
    "                        law_id2=chapter_id\n",
    "                    )\n",
    "\n",
    "            # Handle Clauses inside the Chapter\n",
    "            for cl in chapter_obj.get(\"clauses\", []):\n",
    "                clause_id = f\"{chapter_id}_C_{cl.get('clause', '?')}\"\n",
    "                clause_text = get_text(cl, \"text\")\n",
    "                root = None\n",
    "                ref = None\n",
    "                second_entity = []\n",
    "\n",
    "                dml_ddl_neo4j(\n",
    "                    f\"\"\"\n",
    "                    MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                    SET c.text = $text\n",
    "                    WITH c\n",
    "                    MATCH (ch:Chapter:{ns_label} {{id: $chapter_id}})\n",
    "                    MERGE (ch)-[:HAS_CLAUSE]->(c)\n",
    "                    \"\"\",\n",
    "                    id=clause_id,\n",
    "                    text=clause_text,\n",
    "                    chapter_id=chapter_id,\n",
    "                )\n",
    "                \n",
    "                #Extract relation\n",
    "                clause_text = re.sub(r'^(ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*', '', clause_text, flags=re.IGNORECASE)\n",
    "                re_text = sent_tokenize(clause_text)\n",
    "                for sentence in re_text:\n",
    "                    root = None\n",
    "                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                    if len(re_temp['document_id']) > 0:\n",
    "                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                    if 'nÃ y' in sentence.split():\n",
    "                        words = sentence.split()\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        for i, token in enumerate(words):\n",
    "                            if token == \"nÃ y\" and i > 0:\n",
    "                                prev_word = words[i-1]\n",
    "                                if prev_word.lower() in self_check.keys():\n",
    "                                    text_type = self_check[prev_word.lower()]\n",
    "                                    match text_type:\n",
    "                                        case \"doc1\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Luáº­t\"\n",
    "                                        case \"doc2\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                        case \"doc3\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                        case \"doc4\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                        case \"chapter\":\n",
    "                                            root = chapter_id\n",
    "                                        case \"clause\":\n",
    "                                            root = clause_id\n",
    "                                        case \"point\":\n",
    "                                            root = point_id\n",
    "                                    if root is not None:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                    else:\n",
    "                                        second_entity = []        \n",
    "                    if not second_entity or not relation:\n",
    "                        continue                           \n",
    "                    for entity in second_entity or []:\n",
    "                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                        label = list(entity.keys())[0]       \n",
    "                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                            WITH l\n",
    "                            MATCH (r:Clause:`{ns_label}` {{id: $law_id2}})\n",
    "                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                            \"\"\",\n",
    "                            law_id=label,\n",
    "                            law_id2=clause_id\n",
    "                        )\n",
    "\n",
    "                # Handle Points (1.)\n",
    "                for point in cl.get(\"points\", []):\n",
    "                    point_id = f\"{clause_id}_P_{point.get('point', '?')}\"\n",
    "                    point_text = get_text(point, \"text\")\n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    second_entity = []\n",
    "            \n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                        SET p.text = $text\n",
    "                        WITH p\n",
    "                        MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                        MERGE (c)-[:HAS_POINT]->(p)\n",
    "                        \"\"\",\n",
    "                        id=point_id,\n",
    "                        text=point_text,\n",
    "                        clause_id=clause_id,\n",
    "                    )\n",
    "                    \n",
    "                    #Extract relation\n",
    "                    re_text = sent_tokenize(point_text)\n",
    "                    for sentence in re_text:\n",
    "                        root = None\n",
    "                        re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                        if len(re_temp['document_id']) > 0:\n",
    "                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                        if 'nÃ y' in sentence.split():\n",
    "                            words = sentence.split()\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            for i, token in enumerate(words):\n",
    "                                if token == \"nÃ y\" and i > 0:\n",
    "                                    prev_word = words[i-1]\n",
    "                                    if prev_word.lower() in self_check.keys():\n",
    "                                        text_type = self_check[prev_word.lower()]\n",
    "                                        match text_type:\n",
    "                                            case \"doc1\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Luáº­t\"\n",
    "                                            case \"doc2\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                            case \"doc3\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"ThÃ´ng_tÆ°\"\n",
    "                                            case \"doc4\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                            case \"chapter\":\n",
    "                                                root = chapter_id\n",
    "                                            case \"clause\":\n",
    "                                                root = clause_id\n",
    "                                            case \"point\":\n",
    "                                                root = point_id\n",
    "                                        if root is not None:\n",
    "                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                        else:\n",
    "                                            second_entity = []        \n",
    "                        if not second_entity or not relation:\n",
    "                            continue                             \n",
    "                        for entity in second_entity or []:\n",
    "                            if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                            label = list(entity.keys())[0]       \n",
    "                            ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                WITH l\n",
    "                                MATCH (r:Point:`{ns_label}` {{id: $law_id2}})\n",
    "                                MERGE (r)-[:`{relation}`]->(l)\n",
    "                                \"\"\",\n",
    "                                law_id=label,\n",
    "                                law_id2=point_id\n",
    "                            )\n",
    "\n",
    "                    # Handle Subpoints (a))\n",
    "                    for subpoint in point.get(\"subpoints\", []):\n",
    "                        subpoint_id = f\"{point_id}_SP_{subpoint.get('subpoint', '?')}\"\n",
    "                        subpoint_text = get_text(subpoint, \"text\")\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        second_entity = []\n",
    "                    \n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                            SET sp.text = $text\n",
    "                            WITH sp\n",
    "                            MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                            MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                            \"\"\",\n",
    "                            id=subpoint_id,\n",
    "                            text=subpoint_text,\n",
    "                            point_id=point_id,\n",
    "                        )\n",
    "                        \n",
    "                        #Extract relation\n",
    "                        re_text = sent_tokenize(subpoint_text)\n",
    "                        for sentence in re_text:\n",
    "                            root = None\n",
    "                            re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                            if len(re_temp['document_id']) > 0:\n",
    "                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                            if 'nÃ y' in sentence.split():\n",
    "                                words = sentence.split()\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                for i, token in enumerate(words):\n",
    "                                    if token == \"nÃ y\" and i > 0:\n",
    "                                        prev_word = words[i-1]\n",
    "                                        if prev_word.lower() in self_check.keys():\n",
    "                                            text_type = self_check[prev_word.lower()]\n",
    "                                            match text_type:\n",
    "                                                case \"doc1\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Luáº­t\"\n",
    "                                                case \"doc2\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                case \"doc3\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                case \"doc4\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                case \"chapter\":\n",
    "                                                    root = chapter_id\n",
    "                                                case \"clause\":\n",
    "                                                    root = clause_id\n",
    "                                                case \"point\":\n",
    "                                                    root = point_id\n",
    "                                            if root is not None:\n",
    "                                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                            else:\n",
    "                                                second_entity = []        \n",
    "                            if not second_entity or not relation:\n",
    "                                continue                              \n",
    "                            for entity in second_entity or []:\n",
    "                                if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                                label = list(entity.keys())[0]       \n",
    "                                ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                    WITH l\n",
    "                                    MATCH (r:Subpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                    MERGE (r)-[:`{relation}`]->(l)\n",
    "                                    \"\"\",\n",
    "                                    law_id=label,\n",
    "                                    law_id2=subpoint_id\n",
    "                                )\n",
    "                            \n",
    "                        # Handle SubSubpoints (a.1))\n",
    "                        for ssp in subpoint.get(\"subsubpoints\", []):\n",
    "                            ssp_id = f\"{subpoint_id}_SSP_{ssp.get('subsubpoint', '?')}\"\n",
    "                            ssp_text = get_text(ssp, \"text\")\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            second_entity = []\n",
    "                            \n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (ssp:Subsubpoint:{ns_label} {{id: $id}})\n",
    "                                SET ssp.text = $text\n",
    "                                WITH ssp\n",
    "                                MATCH (sp:Subpoint:{ns_label} {{id: $subpoint_id}})\n",
    "                                MERGE (sp)-[:HAS_SUBSUBPOINT]->(ssp)\n",
    "                                \"\"\",\n",
    "                                id=ssp_id,\n",
    "                                text=ssp_text,\n",
    "                                subpoint_id=subpoint_id,\n",
    "                            )\n",
    "                            \n",
    "                            #Extract relation\n",
    "                            re_text = sent_tokenize(ssp_text)\n",
    "                            for sentence in re_text:\n",
    "                                root = None\n",
    "                                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                if len(re_temp['document_id']) > 0:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                if 'nÃ y' in sentence.split():\n",
    "                                    words = sentence.split()\n",
    "                                    root = None\n",
    "                                    ref = None\n",
    "                                    for i, token in enumerate(words):\n",
    "                                        if token == \"nÃ y\" and i > 0:\n",
    "                                            prev_word = words[i-1]\n",
    "                                            if prev_word.lower() in self_check.keys():\n",
    "                                                text_type = self_check[prev_word.lower()]\n",
    "                                                match text_type:\n",
    "                                                    case \"doc1\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Luáº­t\"\n",
    "                                                    case \"doc2\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                    case \"doc3\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                    case \"doc4\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                    case \"chapter\":\n",
    "                                                        root = chapter_id\n",
    "                                                    case \"clause\":\n",
    "                                                        root = clause_id\n",
    "                                                    case \"point\":\n",
    "                                                        root = point_id\n",
    "                                                if root is not None:\n",
    "                                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                else:\n",
    "                                                    second_entity = []        \n",
    "                                if not second_entity or not relation:\n",
    "                                    continue                               \n",
    "                                for entity in second_entity or []:\n",
    "                                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                                    label = list(entity.keys())[0]       \n",
    "                                    ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                    dml_ddl_neo4j(\n",
    "                                        f\"\"\"\n",
    "                                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                        WITH l\n",
    "                                        MATCH (r:Subsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                                        \"\"\",\n",
    "                                        law_id=label,\n",
    "                                        law_id2=ssp_id\n",
    "                                    )\n",
    "\n",
    "                            # Handle SubSubSubpoints (a.1.1))\n",
    "                            for sssp in ssp.get(\"subsubsubpoints\", []):\n",
    "                                sssp_id = f\"{ssp_id}_SSSP_{sssp.get('subsubsubpoint', '?')}\"\n",
    "                                sssp_text = get_text(sssp, \"text\")\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                second_entity = []\n",
    "                                \n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (sssp:Subsubsubpoint:{ns_label} {{id: $id}})\n",
    "                                    SET sssp.text = $text\n",
    "                                    WITH sssp\n",
    "                                    MATCH (ssp:Subsubpoint:{ns_label} {{id: $ssp_id}})\n",
    "                                    MERGE (ssp)-[:HAS_SUBSUBSUBPOINT]->(sssp)\n",
    "                                    \"\"\",\n",
    "                                    id=sssp_id,\n",
    "                                    text=sssp_text,\n",
    "                                    ssp_id=ssp_id,\n",
    "                                )\n",
    "                                \n",
    "                                #Extract relation\n",
    "                                re_text = sent_tokenize(sssp_text)\n",
    "                                for sentence in re_text:\n",
    "                                    root = None\n",
    "                                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                    if len(re_temp['document_id']) > 0:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                    if 'nÃ y' in sentence.split():\n",
    "                                        words = sentence.split()\n",
    "                                        root = None\n",
    "                                        ref = None\n",
    "                                        for i, token in enumerate(words):\n",
    "                                            if token == \"nÃ y\" and i > 0:\n",
    "                                                prev_word = words[i-1]\n",
    "                                                if prev_word.lower() in self_check.keys():\n",
    "                                                    text_type = self_check[prev_word.lower()]\n",
    "                                                    match text_type:\n",
    "                                                        case \"doc1\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Luáº­t\"\n",
    "                                                        case \"doc2\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                        case \"doc3\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                        case \"doc4\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                        case \"chapter\":\n",
    "                                                            root = chapter_id\n",
    "                                                        case \"clause\":\n",
    "                                                            root = clause_id\n",
    "                                                        case \"point\":\n",
    "                                                            root = point_id\n",
    "                                                    if root is not None:\n",
    "                                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                    else:\n",
    "                                                        second_entity = []        \n",
    "                                    if not second_entity or not relation:\n",
    "                                        continue                                \n",
    "                                    for entity in second_entity or []:\n",
    "                                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                                        label = list(entity.keys())[0]       \n",
    "                                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                        dml_ddl_neo4j(\n",
    "                                            f\"\"\"\n",
    "                                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                            WITH l\n",
    "                                            MATCH (r:Subsubsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                                            \"\"\",\n",
    "                                            law_id=label,\n",
    "                                            law_id2=sssp_id\n",
    "                                        )\n",
    "\n",
    "    # If WITHOUT chapters (only clauses)\n",
    "    elif \"clauses\" in parsed:\n",
    "        for cl in parsed[\"clauses\"]:\n",
    "            clause_id = f\"{metadata['law_id']}_C_{cl.get('clause', '?')}\"\n",
    "            clause_text = get_text(cl, \"text\")\n",
    "            re_text = None\n",
    "            re_temp = None\n",
    "            relation = None\n",
    "            second_entity = []\n",
    "            \n",
    "            dml_ddl_neo4j(\n",
    "                f\"\"\"\n",
    "                MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                SET c.text = $text\n",
    "                WITH c\n",
    "                MATCH (l:{ns_label} {{id: $law_id}})\n",
    "                MERGE (l)-[:HAS_CLAUSE]->(c)\n",
    "                \"\"\",\n",
    "                id=clause_id,\n",
    "                text=clause_text,\n",
    "                law_id=metadata[\"law_id\"],\n",
    "            )\n",
    "\n",
    "            #Extract relation\n",
    "            clause_text = re.sub(r'^(ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*', '', clause_text, flags=re.IGNORECASE)\n",
    "            re_text = sent_tokenize(clause_text)\n",
    "            for sentence in re_text:\n",
    "                root = None\n",
    "                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                if len(re_temp['document_id']) > 0:\n",
    "                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                if 'nÃ y' in sentence.split():\n",
    "                    words = sentence.split()\n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    for i, token in enumerate(words):\n",
    "                        if token == \"nÃ y\" and i > 0:\n",
    "                            prev_word = words[i-1]\n",
    "                            if prev_word.lower() in self_check.keys():\n",
    "                                text_type = self_check[prev_word.lower()]\n",
    "                                match text_type:\n",
    "                                    case \"doc1\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Luáº­t\"\n",
    "                                    case \"doc2\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                    case \"doc3\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                    case \"doc4\":\n",
    "                                        root = metadata[\"law_id\"]\n",
    "                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                    case \"chapter\":\n",
    "                                        root = chapter_id\n",
    "                                    case \"clause\":\n",
    "                                        root = clause_id\n",
    "                                    case \"point\":\n",
    "                                        root = point_id\n",
    "                                if root is not None:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                else:\n",
    "                                    second_entity = []        \n",
    "                if not second_entity or not relation:\n",
    "                    continue                                \n",
    "                for entity in second_entity or []:\n",
    "                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                        continue\n",
    "                    label = list(entity.keys())[0]       \n",
    "                    ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                        WITH l\n",
    "                        MATCH (r:Clause:`{ns_label}` {{id: $law_id2}})\n",
    "                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                        \"\"\",\n",
    "                        law_id=label,\n",
    "                        law_id2=clause_id\n",
    "                    )\n",
    "                \n",
    "            #Handle Point\n",
    "            for point in cl.get(\"points\", []):\n",
    "                point_id = f\"{clause_id}_P_{point.get('point', '?')}\"\n",
    "                point_text = get_text(point, \"text\")\n",
    "                \n",
    "                root = None\n",
    "                ref = None\n",
    "                second_entity = []\n",
    "\n",
    "                dml_ddl_neo4j(\n",
    "                    f\"\"\"\n",
    "                    MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                    SET p.text = $text\n",
    "                    WITH p\n",
    "                    MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                    MERGE (c)-[:HAS_POINT]->(p)\n",
    "                    \"\"\",\n",
    "                    id=point_id,\n",
    "                    text=point_text,\n",
    "                    clause_id=clause_id,\n",
    "                )\n",
    "                \n",
    "                #Extract relation\n",
    "                re_text = sent_tokenize(point_text)\n",
    "                for sentence in re_text:\n",
    "                    root = None\n",
    "                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                    if len(re_temp['document_id']) > 0:\n",
    "                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                    if 'nÃ y' in sentence.split():\n",
    "                        words = sentence.split()\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        for i, token in enumerate(words):\n",
    "                            if token == \"nÃ y\" and i > 0:\n",
    "                                prev_word = words[i-1]\n",
    "                                if prev_word.lower() in self_check.keys():\n",
    "                                    text_type = self_check[prev_word.lower()]\n",
    "                                    match text_type:\n",
    "                                        case \"doc1\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Luáº­t\"\n",
    "                                        case \"doc2\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                        case \"doc3\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                        case \"doc4\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                        case \"chapter\":\n",
    "                                            root = chapter_id\n",
    "                                        case \"clause\":\n",
    "                                            root = clause_id\n",
    "                                        case \"point\":\n",
    "                                            root = point_id\n",
    "                                    if root is not None:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                    else:\n",
    "                                        second_entity = []        \n",
    "                    if not second_entity or not relation:\n",
    "                        continue                               \n",
    "                    for entity in second_entity or []:\n",
    "                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                            continue\n",
    "                        label = list(entity.keys())[0]       \n",
    "                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                            WITH l\n",
    "                            MATCH (r:Point:`{ns_label}` {{id: $law_id2}})\n",
    "                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                            \"\"\",\n",
    "                            law_id=label,\n",
    "                            law_id2=point_id\n",
    "                        )\n",
    "                \n",
    "                #Handle Subpoint\n",
    "                for subpoint in point.get(\"subpoints\", []):\n",
    "                    subpoint_id = f\"{point_id}_SP_{subpoint.get('subpoint', '?')}\"\n",
    "                    subpoint_text = get_text(subpoint, \"text\")\n",
    "                    \n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    second_entity = []\n",
    "                    \n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                        SET sp.text = $text\n",
    "                        WITH sp\n",
    "                        MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                        MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                        \"\"\",\n",
    "                        id=subpoint_id,\n",
    "                        text=subpoint_text,\n",
    "                        point_id=point_id,\n",
    "                    )\n",
    "                    \n",
    "                    #Extract relation\n",
    "                    re_text = sent_tokenize(subpoint_text)\n",
    "                    for sentence in re_text:\n",
    "                        root = None\n",
    "                        re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                        if len(re_temp['document_id']) > 0:\n",
    "                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                        if 'nÃ y' in sentence.split():\n",
    "                            words = sentence.split()\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            for i, token in enumerate(words):\n",
    "                                if token == \"nÃ y\" and i > 0:\n",
    "                                    prev_word = words[i-1]\n",
    "                                    if prev_word.lower() in self_check.keys():\n",
    "                                        text_type = self_check[prev_word.lower()]\n",
    "                                        match text_type:\n",
    "                                            case \"doc1\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Luáº­t\"\n",
    "                                            case \"doc2\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                            case \"doc3\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"ThÃ´ng_tÆ°\"\n",
    "                                            case \"doc4\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                            case \"chapter\":\n",
    "                                                root = chapter_id\n",
    "                                            case \"clause\":\n",
    "                                                root = clause_id\n",
    "                                            case \"point\":\n",
    "                                                root = point_id\n",
    "                                        if root is not None:\n",
    "                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                        else:\n",
    "                                            second_entity = []        \n",
    "                        if not second_entity or not relation:\n",
    "                                    continue                            \n",
    "                        for entity in second_entity or []:\n",
    "                            if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                continue\n",
    "                            label = list(entity.keys())[0]       \n",
    "                            ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                WITH l\n",
    "                                MATCH (r:Subpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                MERGE (r)-[:`{relation}`]->(l)\n",
    "                                \"\"\",\n",
    "                                law_id=label,\n",
    "                                law_id2=subpoint_id\n",
    "                            )\n",
    "                    \n",
    "                    # Handle SubSubpoints (a.1))\n",
    "                    for ssp in subpoint.get(\"subsubpoints\", []):\n",
    "                        ssp_id = f\"{subpoint_id}_SSP_{ssp.get('subsubpoint', '?')}\"\n",
    "                        ssp_text = get_text(ssp, \"text\")\n",
    "                        \n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        second_entity = []\n",
    "\n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (ssp:Subsubpoint:{ns_label} {{id: $id}})\n",
    "                            SET ssp.text = $text\n",
    "                            WITH ssp\n",
    "                            MATCH (sp:Subpoint:{ns_label} {{id: $subpoint_id}})\n",
    "                            MERGE (sp)-[:HAS_SUBSUBPOINT]->(ssp)\n",
    "                            \"\"\",\n",
    "                            id=ssp_id,\n",
    "                            text=ssp_text,\n",
    "                            subpoint_id=subpoint_id,\n",
    "                        )\n",
    "                        \n",
    "                        #Extract relation\n",
    "                        re_text = sent_tokenize(ssp_text)\n",
    "                        for sentence in re_text:\n",
    "                            root = None\n",
    "                            re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                            if len(re_temp['document_id']) > 0:\n",
    "                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                            if 'nÃ y' in sentence.split():\n",
    "                                words = sentence.split()\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                for i, token in enumerate(words):\n",
    "                                    if token == \"nÃ y\" and i > 0:\n",
    "                                        prev_word = words[i-1]\n",
    "                                        if prev_word.lower() in self_check.keys():\n",
    "                                            text_type = self_check[prev_word.lower()]\n",
    "                                            match text_type:\n",
    "                                                case \"doc1\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Luáº­t\"\n",
    "                                                case \"doc2\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                case \"doc3\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                case \"doc4\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                case \"chapter\":\n",
    "                                                    root = chapter_id\n",
    "                                                case \"clause\":\n",
    "                                                    root = clause_id\n",
    "                                                case \"point\":\n",
    "                                                    root = point_id\n",
    "                                            if root is not None:\n",
    "                                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                            else:\n",
    "                                                second_entity = []        \n",
    "                            if not second_entity or not relation:\n",
    "                                    continue                              \n",
    "                            for entity in second_entity or []:\n",
    "                                if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                    continue\n",
    "                                label = list(entity.keys())[0]       \n",
    "                                ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "                                \n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                    WITH l\n",
    "                                    MATCH (r:Subsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                    MERGE (r)-[:`{relation}`]->(l)\n",
    "                                    \"\"\",\n",
    "                                    law_id=label,\n",
    "                                    law_id2=ssp_id\n",
    "                                )\n",
    "\n",
    "                        # Handle SubSubSubpoints (a.1.1))\n",
    "                        for sssp in ssp.get(\"subsubsubpoints\", []):\n",
    "                            sssp_id = f\"{ssp_id}_SSSP_{sssp.get('subsubsubpoint', '?')}\"\n",
    "                            sssp_text = get_text(sssp, \"text\")\n",
    "                            \n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            second_entity = []\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (sssp:Subsubsubpoint:{ns_label} {{id: $id}})\n",
    "                                SET sssp.text = $text\n",
    "                                WITH sssp\n",
    "                                MATCH (ssp:Subsubpoint:{ns_label} {{id: $ssp_id}})\n",
    "                                MERGE (ssp)-[:HAS_SUBSUBSUBPOINT]->(sssp)\n",
    "                                \"\"\",\n",
    "                                id=sssp_id,\n",
    "                                text=sssp_text,\n",
    "                                ssp_id=ssp_id,\n",
    "                            )\n",
    "                            \n",
    "                            #Extract relation\n",
    "                            re_text = sent_tokenize(sssp_text)\n",
    "                            for sentence in re_text:\n",
    "                                root = None\n",
    "                                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                if len(re_temp['document_id']) > 0:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                if 'nÃ y' in sentence.split():\n",
    "                                    words = sentence.split()\n",
    "                                    root = None\n",
    "                                    ref = None\n",
    "                                    for i, token in enumerate(words):\n",
    "                                        if token == \"nÃ y\" and i > 0:\n",
    "                                            prev_word = words[i-1]\n",
    "                                            if prev_word.lower() in self_check.keys():\n",
    "                                                text_type = self_check[prev_word.lower()]\n",
    "                                                match text_type:\n",
    "                                                    case \"doc1\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Luáº­t\"\n",
    "                                                    case \"doc2\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                    case \"doc3\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                    case \"doc4\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                    case \"chapter\":\n",
    "                                                        root = chapter_id\n",
    "                                                    case \"clause\":\n",
    "                                                        root = clause_id\n",
    "                                                    case \"point\":\n",
    "                                                        root = point_id\n",
    "                                                if root is not None:\n",
    "                                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                else:\n",
    "                                                    second_entity = []        \n",
    "                                if not second_entity or not relation:\n",
    "                                    continue                          \n",
    "                                for entity in second_entity or []:\n",
    "                                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                                    label = list(entity.keys())[0]       \n",
    "                                    ref_type = ref if list(entity.values())[0] == \"Document\" else list(entity.values())[0] \n",
    "\n",
    "                                    dml_ddl_neo4j(\n",
    "                                        f\"\"\"\n",
    "                                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                        WITH l\n",
    "                                        MATCH (r:Subsubsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                                        \"\"\",\n",
    "                                        law_id=label,\n",
    "                                        law_id2=sssp_id\n",
    "                                    )\n",
    "\n",
    "    # Cleanup temporary \"no_chapter\" node\n",
    "    dml_ddl_neo4j(\n",
    "        f\"\"\"\n",
    "        MATCH (ch:Chapter:{ns_label})\n",
    "        WHERE ch.id ENDS WITH \"_no_chapter\"\n",
    "        DETACH DELETE ch\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc13529",
   "metadata": {},
   "source": [
    "### Test Text (sample for 2 documents of different structure hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb3090e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "QUá»C Há»˜I  \n",
    "-------  Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM  \n",
    "Äá»™c láº­p - Tá»± do - Háº¡nh phÃºc  \n",
    "---------------  \n",
    "Luáº­t sá»‘: 23/2026/QH  HÃ  Ná»™i, ngÃ y 20 thÃ¡ng 1 nÄƒm 2026\n",
    "  \n",
    "LUáº¬T  \n",
    "THUáº¾ Thu nháº­p Máº«u\n",
    "CÄƒn cá»© Hiáº¿n phÃ¡p nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam; \n",
    "CÄƒn cá»© Luáº­t thuáº¿ Thu Nháº­p Doanh Nghiá»‡p sá»‘ 67/2025/QH15;\n",
    "Quá»‘c há»™i ban hÃ nh Luáº­t Thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh  \n",
    "Luáº­t nÃ y quy Ä‘á»‹nh vá» ngÆ°á»i ná»™p thuáº¿, thu nháº­p chá»‹u thuáº¿, thu nháº­p Ä‘Æ°á»£c miá»…n thuáº¿, cÄƒn cá»© tÃ­nh \n",
    "thuáº¿, phÆ°Æ¡ng phÃ¡p tÃ­nh thuáº¿ vÃ  Æ°u Ä‘Ã£i thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "1.CÃ¡c quy Ä‘á»‹nh Ä‘Æ°á»£c liá»‡t kÃª táº¡i Äiá»u nÃ y Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ báº±ng cÃ¡c Ä‘iá»u má»›i.\n",
    "\n",
    "a) CÃ¡c quy Ä‘á»‹nh táº¡i khoáº£n 1 Äiá»u 1 Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ bÃªn dÆ°á»›i.\n",
    "\n",
    "a.1) CÃ¡c quy Ä‘á»‹nh táº¡i Äiá»ƒm a) khoáº£n 1 Ä‘iá»u nÃ y Ä‘Æ°á»£c bá»• sung.\n",
    "\n",
    "a.1.1) CÃ¡c quy Ä‘á»‹nh táº¡i Ä‘iá»ƒm a.1) khoáº£n 1 Äiá»u nÃ y Ä‘Æ°á»£c sá»­a Ä‘á»•i.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e26c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "QUá»C Há»˜I  \n",
    "-------  Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM  \n",
    "Äá»™c láº­p - Tá»± do - Háº¡nh phÃºc  \n",
    "---------------  \n",
    "Luáº­t sá»‘: 67/2025/QH15  HÃ  Ná»™i, ngÃ y 14 thÃ¡ng 6 nÄƒm 2025  \n",
    "  \n",
    "LUáº¬T  \n",
    "THUáº¾ THU NHáº¬P DOANH NGHIá»†P  \n",
    "CÄƒn cá»© Hiáº¿n phÃ¡p nÆ°á»›c Cá»™ng hÃ²a xÃ£ há»™i chá»§ nghÄ©a Viá»‡t Nam;  \n",
    "Quá»‘c há»™i ban hÃ nh Luáº­t Thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "ChÆ°Æ¡ng I  \n",
    "\n",
    "NHá»®NG QUY Äá»ŠNH CHUNG. \n",
    "\n",
    "Sá»­a Ä‘á»•i bá»• sung Ä‘iá»ƒm a Ä‘áº¿n Ä‘iá»ƒm e Ä‘iá»u 32 hoáº·c Ä‘iá»u 35, Ä‘iá»ƒm b.1.1), \n",
    "Ä‘iá»ƒm c.1), Ä‘iá»ƒm d.1) hoáº·c Ä‘iá»ƒm e.2) khoáº£n 2 Ä‘iá»u 1, khoáº£n 3 Ä‘iá»u 10 luáº­t sá»‘ 20/2019/QH14.\n",
    "\n",
    "Há»§y bá» Luáº­t nÃ y vÃ  cÃ¡c Ä‘iá»u liÃªn quan Ä‘áº¿n nÃ³.\n",
    "\n",
    "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh  \n",
    "Luáº­t nÃ y quy Ä‘á»‹nh vá» ngÆ°á»i ná»™p thuáº¿, thu nháº­p chá»‹u thuáº¿, thu nháº­p Ä‘Æ°á»£c miá»…n thuáº¿, cÄƒn cá»© tÃ­nh \n",
    "thuáº¿, phÆ°Æ¡ng phÃ¡p tÃ­nh thuáº¿ vÃ  Æ°u Ä‘Ã£i thuáº¿ thu nháº­p doanh nghiá»‡p.  \n",
    "\n",
    "Sá»­a Ä‘á»•i bá»‘ sung ChÆ°Æ¡ng nÃ y vÃ  cÃ¡c Ä‘iá»u liÃªn quan.\n",
    "\n",
    "1.CÃ¡c quy Ä‘á»‹nh Ä‘Æ°á»£c liá»‡t kÃª táº¡i Äiá»u nÃ y Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ báº±ng cÃ¡c Ä‘iá»u má»›i.\n",
    "\n",
    "a) CÃ¡c quy Ä‘á»‹nh táº¡i khoáº£n 1 Äiá»u 1 chÆ°Æ¡ng nÃ y Ä‘Æ°á»£c há»§y bá» vÃ  thay tháº¿ bÃªn dÆ°á»›i.\n",
    "\n",
    "a.1) CÃ¡c quy Ä‘á»‹nh táº¡i Äiá»ƒm a) khoáº£n 1 Ä‘iá»u nÃ y Ä‘Æ°á»£c bá»• sung.\n",
    "\n",
    "a.1.1) CÃ¡c quy Ä‘á»‹nh táº¡i Ä‘iá»ƒm a.1) khoáº£n 1 Äiá»u 1 chÆ°Æ¡ng nÃ y Ä‘Æ°á»£c sá»­a Ä‘á»•i.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_neo4j(text, namespace=\"Test\"):\n",
    "    # Extract metadata\n",
    "        df_meta = ner.extract_document_metadata(text)\n",
    "        meta_row = df_meta.iloc[0]\n",
    "        df_relation = final_re.final_relation(text)\n",
    "        \n",
    "        #Flag for amend document or not\n",
    "        amend = df_meta['amend'].iloc[0]\n",
    "        \n",
    "        _check = {'luáº­t': 'doc1', 'Ä‘á»‹nh': 'doc2', 'tÆ°': 'doc3', 'quyáº¿t': 'doc4', 'chÆ°Æ¡ng': 'chapter', 'Ä‘iá»u': 'clause', 'má»¥c': 'point'}\n",
    "\n",
    "        metadata = {\n",
    "            \"law_id\": meta_row[\"document_id\"],\n",
    "            \"title\": meta_row[\"title\"],\n",
    "            \"issuer\": meta_row[\"issuer\"],\n",
    "            \"issue_date\": meta_row[\"issue_date\"],\n",
    "            \"location\": meta_row[\"location\"],\n",
    "            \"issuer_department\": meta_row[\"issuer_department\"],\n",
    "            \"document_type\": meta_row[\"document_type\"],\n",
    "            \"amend\": meta_row['amend']\n",
    "        }\n",
    "\n",
    "        # Document type\n",
    "        doc_type_label = metadata[\"document_type\"].replace(\" \", \"_\").capitalize()\n",
    "        # Namespace\n",
    "        ns_label = re.sub(r\"\\W+\", \"_\", namespace)\n",
    "\n",
    "        # central document node\n",
    "        dml_ddl_neo4j(\n",
    "            f\"\"\"\n",
    "            MERGE (l:`{doc_type_label}`:`{ns_label}` {{id: $law_id}})\n",
    "            SET l += $meta\n",
    "            \"\"\",\n",
    "            law_id=metadata[\"law_id\"],\n",
    "            meta=metadata,\n",
    "        )\n",
    "        \n",
    "        #Connect reference node\n",
    "        for i in range(len(df_relation)):\n",
    "            \n",
    "            doc_type = df_relation.iloc[i,7].replace(\" \", \"_\").capitalize()\n",
    "            \n",
    "            amended = (df_relation.iloc[i,2] == 'Amended')\n",
    "            \n",
    "            if ((len(df_relation.iloc[i,6].split('/')) > 1) or (df_relation.iloc[i, 6] == 'HP')) and (df_relation.iloc[i,7]):\n",
    "                \n",
    "                dml_ddl_neo4j(\n",
    "                f\"\"\"\n",
    "                MERGE (l:`{doc_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                WITH l\n",
    "                MATCH (r: `{doc_type_label}`:`{ns_label}` {{id: $law_id2}})\n",
    "                MERGE (r)-[:`{df_relation.iloc[i,2]}`]->(l)\n",
    "                \"\"\",\n",
    "                law_id=df_relation.iloc[i,6],\n",
    "                law_id2=metadata['law_id']\n",
    "            )\n",
    "                if amended:\n",
    "                    \n",
    "                    #Save amended document ID to be used as root for later\n",
    "                    root_amend = df_relation.iloc[i, 6]\n",
    "                    \n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (l:`{doc_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                        SET l.last_updated = $update_date\n",
    "                        \"\"\",\n",
    "                        law_id=df_relation.iloc[i,6],\n",
    "                        update_date = metadata['issue_date']\n",
    "                    )\n",
    "                if (df_relation.iloc[i,4]):  \n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MATCH (l:`{doc_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                        SET l.issue_date = $issue_date\n",
    "                        \"\"\",\n",
    "                        law_id=df_relation.iloc[i,6],\n",
    "                        issue_date=str(df_relation.iloc[i,4])\n",
    "                    )\n",
    "                    \n",
    "            #if id not available, use date\n",
    "            if df_relation.iloc[i,4]:\n",
    "                dml_ddl_neo4j(\n",
    "                f\"\"\"\n",
    "                MERGE (l:`{doc_type}`:`{ns_label}` {{issue_date: $issue_date}})\n",
    "                WITH l\n",
    "                MATCH (r: `{doc_type_label}`:`{ns_label}` {{id: $law_id2}})\n",
    "                MERGE (r)-[:`{df_relation.iloc[i,2]}`]->(l)\n",
    "                \"\"\",\n",
    "                issue_date=str(df_relation.iloc[i,4]),\n",
    "                law_id2=metadata['law_id']\n",
    "            )\n",
    "                \n",
    "        # Parse structure\n",
    "        parsed = parse_legal_text(text)\n",
    "\n",
    "        # Extract text\n",
    "        def get_text(node, *keys):\n",
    "            for k in keys:\n",
    "                if isinstance(node, dict) and k in node and node[k]:\n",
    "                    return node[k]\n",
    "            if isinstance(node, str):\n",
    "                return node\n",
    "            return \"\"\n",
    "\n",
    "        # If HAS chapters \n",
    "        if \"chapters\" in parsed:\n",
    "            for chapter_key, chapter_obj in parsed[\"chapters\"].items():\n",
    "                chapter_id = f\"{metadata['law_id']}_{chapter_key.replace(' ', '_')}\"\n",
    "                chapter_title = get_text(chapter_obj, \"title\")\n",
    "                chapter_text = get_text(chapter_obj, \"text\")\n",
    "                re_text = None\n",
    "                re_temp = None\n",
    "                relation = None\n",
    "                second_entity = []\n",
    "                            \n",
    "                dml_ddl_neo4j(\n",
    "                    f\"\"\"\n",
    "                    MERGE (ch:Chapter:{ns_label} {{id: $id}})\n",
    "                    SET ch.title = $title, ch.text = $text\n",
    "                    WITH ch\n",
    "                    MATCH (l:{ns_label} {{law_id: $law_id}})\n",
    "                    MERGE (l)-[:HAS_CHAPTER]->(ch)\n",
    "                    \"\"\",\n",
    "                    id=chapter_id,\n",
    "                    title=chapter_title,\n",
    "                    text=chapter_text,\n",
    "                    law_id=metadata[\"law_id\"],\n",
    "                )\n",
    "                        \n",
    "                #Extract relation\n",
    "                chapter_text = re.sub(r'^(?:ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*|^[a-z](?:\\.\\d+)*\\)', '', chapter_text, flags=re.IGNORECASE)\n",
    "                re_text = sent_tokenize(chapter_text)\n",
    "                # re_text[0] = re_text[0].split(chapter_id.split('_')[-1], 1)[1].strip()\n",
    "                for sentence in re_text:\n",
    "                    root = None\n",
    "                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                    if len(re_temp['document_id']) > 0:\n",
    "                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                    if 'nÃ y' in sentence.split():\n",
    "                        words = sentence.split()\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        for i, token in enumerate(words):\n",
    "                            if token == \"nÃ y\" and i > 0:\n",
    "                                prev_word = words[i-1]\n",
    "                                if prev_word.lower() in _check.keys():\n",
    "                                    text_type = _check[prev_word.lower()]\n",
    "                                    match text_type:\n",
    "                                        case \"doc1\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Luáº­t\"\n",
    "                                        case \"doc2\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                        case \"doc3\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                        case \"doc4\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                        case \"chapter\":\n",
    "                                            root = chapter_id\n",
    "                                        case \"clause\":\n",
    "                                            root = clause_id\n",
    "                                        case \"point\":\n",
    "                                            root = point_id\n",
    "                                    if root is not None:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                    else:\n",
    "                                        second_entity = []   \n",
    "                                            \n",
    "                    if not second_entity or not relation:\n",
    "                        continue               \n",
    "                    for entity in second_entity or []:\n",
    "                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                        label = list(entity.keys())[0]       \n",
    "                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                            WITH l\n",
    "                            MATCH (r:Chapter:`{ns_label}` {{id: $law_id2}})\n",
    "                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                            \"\"\",\n",
    "                            law_id=label,\n",
    "                            law_id2=chapter_id\n",
    "                        )\n",
    "\n",
    "                # Handle Clauses inside the Chapter\n",
    "                for cl in chapter_obj.get(\"clauses\", []):\n",
    "                    clause_id = f\"{chapter_id}_C_{cl.get('clause', '?')}\"\n",
    "                    clause_text = get_text(cl, \"text\")\n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    second_entity = []\n",
    "\n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                        SET c.original_text = $text\n",
    "                        SET c.new_text = $new_text\n",
    "                        WITH c\n",
    "                        MATCH (ch:Chapter:{ns_label} {{id: $chapter_id}})\n",
    "                        MERGE (ch)-[:HAS_CLAUSE]->(c)\n",
    "                        \"\"\",\n",
    "                        id=clause_id,\n",
    "                        text=clause_text,\n",
    "                        chapter_id=chapter_id,\n",
    "                        new_text = None\n",
    "                    )\n",
    "                    \n",
    "                    #Extract relation\n",
    "                    clause_text = re.sub(r'^(?:ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*|^[a-z](?:\\.\\d+)*\\)', '', clause_text, flags=re.IGNORECASE)\n",
    "                    re_text = sent_tokenize(clause_text)\n",
    "                    for sentence in re_text:\n",
    "                        root = None\n",
    "                        re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                        if len(re_temp['document_id']) > 0:\n",
    "                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                        if 'nÃ y' in sentence.split():\n",
    "                            words = sentence.split()\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            for i, token in enumerate(words):\n",
    "                                if token == \"nÃ y\" and i > 0:\n",
    "                                    prev_word = words[i-1]\n",
    "                                    if prev_word.lower() in _check.keys():\n",
    "                                        text_type = _check[prev_word.lower()]\n",
    "                                        match text_type:\n",
    "                                            case \"doc1\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Luáº­t\"\n",
    "                                            case \"doc2\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                            case \"doc3\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"ThÃ´ng_tÆ°\"\n",
    "                                            case \"doc4\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                            case \"chapter\":\n",
    "                                                root = chapter_id\n",
    "                                            case \"clause\":\n",
    "                                                root = clause_id\n",
    "                                            case \"point\":\n",
    "                                                root = point_id\n",
    "                                        if root is not None:\n",
    "                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                        else:\n",
    "                                            second_entity = []        \n",
    "                        if not second_entity or not relation:\n",
    "                            continue                           \n",
    "                        for entity in second_entity or []:\n",
    "                            if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                            label = list(entity.keys())[0]       \n",
    "                            ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                WITH l\n",
    "                                MATCH (r:Clause:`{ns_label}` {{id: $law_id2}})\n",
    "                                MERGE (r)-[:`{relation}`]->(l)\n",
    "                                \"\"\",\n",
    "                                law_id=label,\n",
    "                                law_id2=clause_id\n",
    "                            )\n",
    "\n",
    "                    # Handle Points (1.)\n",
    "                    for point in cl.get(\"points\", []):\n",
    "                        point_id = f\"{clause_id}_P_{point.get('point', '?')}\"\n",
    "                        point_text = get_text(point, \"text\")\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        second_entity = []\n",
    "                \n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                            SET p.original_text = $text\n",
    "                            SET p.new_text = $new_text\n",
    "                            WITH p\n",
    "                            MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                            MERGE (c)-[:HAS_POINT]->(p)\n",
    "                            \"\"\",\n",
    "                            id=point_id,\n",
    "                            text=point_text,\n",
    "                            clause_id=clause_id,\n",
    "                            new_text = None\n",
    "                        )\n",
    "                        \n",
    "                        #Extract relation\n",
    "                        re_text = sent_tokenize(point_text)\n",
    "                        for sentence in re_text:\n",
    "                            root = None\n",
    "                            re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                            if len(re_temp['document_id']) > 0:\n",
    "                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                            if 'nÃ y' in sentence.split():\n",
    "                                words = sentence.split()\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                for i, token in enumerate(words):\n",
    "                                    if token == \"nÃ y\" and i > 0:\n",
    "                                        prev_word = words[i-1]\n",
    "                                        if prev_word.lower() in _check.keys():\n",
    "                                            text_type = _check[prev_word.lower()]\n",
    "                                            match text_type:\n",
    "                                                case \"doc1\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Luáº­t\"\n",
    "                                                case \"doc2\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                case \"doc3\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                case \"doc4\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                case \"chapter\":\n",
    "                                                    root = chapter_id\n",
    "                                                case \"clause\":\n",
    "                                                    root = clause_id\n",
    "                                                case \"point\":\n",
    "                                                    root = point_id\n",
    "                                            if root is not None:\n",
    "                                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                            else:\n",
    "                                                second_entity = []        \n",
    "                            if not second_entity or not relation:\n",
    "                                continue                             \n",
    "                            for entity in second_entity or []:\n",
    "                                if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                                label = list(entity.keys())[0]       \n",
    "                                ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                    WITH l\n",
    "                                    MATCH (r:Point:`{ns_label}` {{id: $law_id2}})\n",
    "                                    MERGE (r)-[:`{relation}`]->(l)\n",
    "                                    \"\"\",\n",
    "                                    law_id=label,\n",
    "                                    law_id2=point_id\n",
    "                                )\n",
    "\n",
    "                        # Handle Subpoints (a))\n",
    "                        for subpoint in point.get(\"subpoints\", []):\n",
    "                            subpoint_id = f\"{point_id}_SP_{subpoint.get('subpoint', '?')}\"\n",
    "                            subpoint_text = get_text(subpoint, \"text\")\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            second_entity = []\n",
    "                        \n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                                SET sp.original_text = $text\n",
    "                                SET sp.new_text = $new_text\n",
    "                                WITH sp\n",
    "                                MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                                MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                                \"\"\",\n",
    "                                id=subpoint_id,\n",
    "                                text=subpoint_text,\n",
    "                                point_id=point_id,\n",
    "                                new_text = None\n",
    "                            )\n",
    "                            \n",
    "                            #Extract relation\n",
    "                            re_text = sent_tokenize(subpoint_text)\n",
    "                            for sentence in re_text:\n",
    "                                root = None\n",
    "                                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                if len(re_temp['document_id']) > 0:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                if 'nÃ y' in sentence.split():\n",
    "                                    words = sentence.split()\n",
    "                                    root = None\n",
    "                                    ref = None\n",
    "                                    for i, token in enumerate(words):\n",
    "                                        if token == \"nÃ y\" and i > 0:\n",
    "                                            prev_word = words[i-1]\n",
    "                                            if prev_word.lower() in _check.keys():\n",
    "                                                text_type = _check[prev_word.lower()]\n",
    "                                                match text_type:\n",
    "                                                    case \"doc1\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Luáº­t\"\n",
    "                                                    case \"doc2\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                    case \"doc3\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                    case \"doc4\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                    case \"chapter\":\n",
    "                                                        root = chapter_id\n",
    "                                                    case \"clause\":\n",
    "                                                        root = clause_id\n",
    "                                                    case \"point\":\n",
    "                                                        root = point_id\n",
    "                                                if root is not None:\n",
    "                                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                else:\n",
    "                                                    second_entity = []        \n",
    "                                if not second_entity or not relation:\n",
    "                                    continue                              \n",
    "                                for entity in second_entity or []:\n",
    "                                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                                    label = list(entity.keys())[0]       \n",
    "                                    ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                    dml_ddl_neo4j(\n",
    "                                        f\"\"\"\n",
    "                                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                        WITH l\n",
    "                                        MATCH (r:Subpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                                        \"\"\",\n",
    "                                        law_id=label,\n",
    "                                        law_id2=subpoint_id\n",
    "                                    )\n",
    "                                \n",
    "                            # Handle SubSubpoints (a.1))\n",
    "                            for ssp in subpoint.get(\"subsubpoints\", []):\n",
    "                                ssp_id = f\"{subpoint_id}_SSP_{ssp.get('subsubpoint', '?')}\"\n",
    "                                ssp_text = get_text(ssp, \"text\")\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                second_entity = []\n",
    "                                \n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (ssp:Subsubpoint:{ns_label} {{id: $id}})\n",
    "                                    SET ssp.original_text = $text\n",
    "                                    SET ssp.new_text = $new_text\n",
    "                                    WITH ssp\n",
    "                                    MATCH (sp:Subpoint:{ns_label} {{id: $subpoint_id}})\n",
    "                                    MERGE (sp)-[:HAS_SUBSUBPOINT]->(ssp)\n",
    "                                    \"\"\",\n",
    "                                    id=ssp_id,\n",
    "                                    text=ssp_text,\n",
    "                                    subpoint_id=subpoint_id,\n",
    "                                    new_text = None\n",
    "                                )\n",
    "                                \n",
    "                                #Extract relation\n",
    "                                re_text = sent_tokenize(ssp_text)\n",
    "                                for sentence in re_text:\n",
    "                                    root = None\n",
    "                                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                    if len(re_temp['document_id']) > 0:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                    if 'nÃ y' in sentence.split():\n",
    "                                        words = sentence.split()\n",
    "                                        root = None\n",
    "                                        ref = None\n",
    "                                        for i, token in enumerate(words):\n",
    "                                            if token == \"nÃ y\" and i > 0:\n",
    "                                                prev_word = words[i-1]\n",
    "                                                if prev_word.lower() in _check.keys():\n",
    "                                                    text_type = _check[prev_word.lower()]\n",
    "                                                    match text_type:\n",
    "                                                        case \"doc1\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Luáº­t\"\n",
    "                                                        case \"doc2\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                        case \"doc3\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                        case \"doc4\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                        case \"chapter\":\n",
    "                                                            root = chapter_id\n",
    "                                                        case \"clause\":\n",
    "                                                            root = clause_id\n",
    "                                                        case \"point\":\n",
    "                                                            root = point_id\n",
    "                                                    if root is not None:\n",
    "                                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                    else:\n",
    "                                                        second_entity = []        \n",
    "                                    if not second_entity or not relation:\n",
    "                                        continue                               \n",
    "                                    for entity in second_entity or []:\n",
    "                                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                                        label = list(entity.keys())[0]       \n",
    "                                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                        dml_ddl_neo4j(\n",
    "                                            f\"\"\"\n",
    "                                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                            WITH l\n",
    "                                            MATCH (r:Subsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                                            \"\"\",\n",
    "                                            law_id=label,\n",
    "                                            law_id2=ssp_id\n",
    "                                        )\n",
    "\n",
    "                                # Handle SubSubSubpoints (a.1.1))\n",
    "                                for sssp in ssp.get(\"subsubsubpoints\", []):\n",
    "                                    sssp_id = f\"{ssp_id}_SSSP_{sssp.get('subsubsubpoint', '?')}\"\n",
    "                                    sssp_text = get_text(sssp, \"text\")\n",
    "                                    root = None\n",
    "                                    ref = None\n",
    "                                    second_entity = []\n",
    "                                    \n",
    "                                    dml_ddl_neo4j(\n",
    "                                        f\"\"\"\n",
    "                                        MERGE (sssp:Subsubsubpoint:{ns_label} {{id: $id}})\n",
    "                                        SET sssp.original_text = $text\n",
    "                                        SET sssp.new_text = $new_text\n",
    "                                        WITH sssp\n",
    "                                        MATCH (ssp:Subsubpoint:{ns_label} {{id: $ssp_id}})\n",
    "                                        MERGE (ssp)-[:HAS_SUBSUBSUBPOINT]->(sssp)\n",
    "                                        \"\"\",\n",
    "                                        id=sssp_id,\n",
    "                                        text=sssp_text,\n",
    "                                        ssp_id=ssp_id,\n",
    "                                        new_text = None\n",
    "                                    )\n",
    "                                    \n",
    "                                    #Extract relation\n",
    "                                    re_text = sent_tokenize(sssp_text)\n",
    "                                    for sentence in re_text:\n",
    "                                        root = None\n",
    "                                        re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                        if len(re_temp['document_id']) > 0:\n",
    "                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                        if 'nÃ y' in sentence.split():\n",
    "                                            words = sentence.split()\n",
    "                                            root = None\n",
    "                                            ref = None\n",
    "                                            for i, token in enumerate(words):\n",
    "                                                if token == \"nÃ y\" and i > 0:\n",
    "                                                    prev_word = words[i-1]\n",
    "                                                    if prev_word.lower() in _check.keys():\n",
    "                                                        text_type = _check[prev_word.lower()]\n",
    "                                                        match text_type:\n",
    "                                                            case \"doc1\":\n",
    "                                                                root = metadata[\"law_id\"]\n",
    "                                                                ref = \"Luáº­t\"\n",
    "                                                            case \"doc2\":\n",
    "                                                                root = metadata[\"law_id\"]\n",
    "                                                                ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                            case \"doc3\":\n",
    "                                                                root = metadata[\"law_id\"]\n",
    "                                                                ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                            case \"doc4\":\n",
    "                                                                root = metadata[\"law_id\"]\n",
    "                                                                ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                            case \"chapter\":\n",
    "                                                                root = chapter_id\n",
    "                                                            case \"clause\":\n",
    "                                                                root = clause_id\n",
    "                                                            case \"point\":\n",
    "                                                                root = point_id\n",
    "                                                        if root is not None:\n",
    "                                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                        else:\n",
    "                                                            second_entity = []        \n",
    "                                        if not second_entity or not relation:\n",
    "                                            continue                                \n",
    "                                        for entity in second_entity or []:\n",
    "                                            if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                                continue\n",
    "                                            label = list(entity.keys())[0]       \n",
    "                                            ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                            dml_ddl_neo4j(\n",
    "                                                f\"\"\"\n",
    "                                                MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                                WITH l\n",
    "                                                MATCH (r:Subsubsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                                MERGE (r)-[:`{relation}`]->(l)\n",
    "                                                \"\"\",\n",
    "                                                law_id=label,\n",
    "                                                law_id2=sssp_id\n",
    "                                            )\n",
    "\n",
    "        # If WITHOUT chapters (only clauses)\n",
    "        elif \"clauses\" in parsed:\n",
    "            for cl in parsed[\"clauses\"]:\n",
    "                clause_id = f\"{metadata['law_id']}_C_{cl.get('clause', '?')}\"\n",
    "                clause_text = get_text(cl, \"text\")\n",
    "                re_text = None\n",
    "                re_temp = None\n",
    "                relation = None\n",
    "                second_entity = []\n",
    "                \n",
    "                dml_ddl_neo4j(\n",
    "                    f\"\"\"\n",
    "                    MERGE (c:Clause:{ns_label} {{id: $id}})\n",
    "                    SET c.original_text = $text\n",
    "                    SET c.new_text = $new_text\n",
    "                    WITH c\n",
    "                    MATCH (l:{ns_label} {{id: $law_id}})\n",
    "                    MERGE (l)-[:HAS_CLAUSE]->(c)\n",
    "                    \"\"\",\n",
    "                    id=clause_id,\n",
    "                    text=clause_text,\n",
    "                    law_id=metadata[\"law_id\"],\n",
    "                    new_text = None\n",
    "                )\n",
    "\n",
    "                #Extract relation\n",
    "                clause_text = re.sub(r'^(?:ChÆ°Æ¡ng|Äiá»u)\\s*\\d*\\s*|^[a-z](?:\\.\\d+)*\\)', '', clause_text, flags=re.IGNORECASE)\n",
    "                re_text = sent_tokenize(clause_text)\n",
    "                for sentence in re_text:\n",
    "                    root = None\n",
    "                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                    if len(re_temp['document_id']) > 0:\n",
    "                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                    if 'nÃ y' in sentence.split():\n",
    "                        words = sentence.split()\n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        for i, token in enumerate(words):\n",
    "                            if token == \"nÃ y\" and i > 0:\n",
    "                                prev_word = words[i-1]\n",
    "                                if prev_word.lower() in _check.keys():\n",
    "                                    text_type = _check[prev_word.lower()]\n",
    "                                    match text_type:\n",
    "                                        case \"doc1\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Luáº­t\"\n",
    "                                        case \"doc2\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                        case \"doc3\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                        case \"doc4\":\n",
    "                                            root = metadata[\"law_id\"]\n",
    "                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                        case \"chapter\":\n",
    "                                            root = chapter_id\n",
    "                                        case \"clause\":\n",
    "                                            root = clause_id\n",
    "                                        case \"point\":\n",
    "                                            root = point_id\n",
    "                                    if root is not None:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                    else:\n",
    "                                        second_entity = []        \n",
    "                    if not second_entity or not relation:\n",
    "                        continue                                \n",
    "                    for entity in second_entity or []:\n",
    "                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                            continue\n",
    "                        label = list(entity.keys())[0]       \n",
    "                        ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                            WITH l\n",
    "                            MATCH (r:Clause:`{ns_label}` {{id: $law_id2}})\n",
    "                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                            \"\"\",\n",
    "                            law_id=label,\n",
    "                            law_id2=clause_id\n",
    "                        )\n",
    "                    \n",
    "                #Handle Point\n",
    "                for point in cl.get(\"points\", []):\n",
    "                    point_id = f\"{clause_id}_P_{point.get('point', '?')}\"\n",
    "                    point_text = get_text(point, \"text\")\n",
    "                    \n",
    "                    root = None\n",
    "                    ref = None\n",
    "                    second_entity = []\n",
    "\n",
    "                    dml_ddl_neo4j(\n",
    "                        f\"\"\"\n",
    "                        MERGE (p:Point:{ns_label} {{id: $id}})\n",
    "                        SET p.original_text = $text\n",
    "                        SET p.new_text = $new_text\n",
    "                        WITH p\n",
    "                        MATCH (c:Clause:{ns_label} {{id: $clause_id}})\n",
    "                        MERGE (c)-[:HAS_POINT]->(p)\n",
    "                        \"\"\",\n",
    "                        id=point_id,\n",
    "                        text=point_text,\n",
    "                        clause_id=clause_id,\n",
    "                        new_text = None\n",
    "                    )\n",
    "                    \n",
    "                    #Extract relation\n",
    "                    re_text = sent_tokenize(point_text)\n",
    "                    for sentence in re_text:\n",
    "                        root = None\n",
    "                        re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                        if len(re_temp['document_id']) > 0:\n",
    "                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                        if 'nÃ y' in sentence.split():\n",
    "                            words = sentence.split()\n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            for i, token in enumerate(words):\n",
    "                                if token == \"nÃ y\" and i > 0:\n",
    "                                    prev_word = words[i-1]\n",
    "                                    if prev_word.lower() in _check.keys():\n",
    "                                        text_type = _check[prev_word.lower()]\n",
    "                                        match text_type:\n",
    "                                            case \"doc1\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Luáº­t\"\n",
    "                                            case \"doc2\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                            case \"doc3\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"ThÃ´ng_tÆ°\"\n",
    "                                            case \"doc4\":\n",
    "                                                root = metadata[\"law_id\"]\n",
    "                                                ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                            case \"chapter\":\n",
    "                                                root = chapter_id\n",
    "                                            case \"clause\":\n",
    "                                                root = clause_id\n",
    "                                            case \"point\":\n",
    "                                                root = point_id\n",
    "                                        if root is not None:\n",
    "                                            _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                        else:\n",
    "                                            second_entity = []        \n",
    "                        if not second_entity or not relation:\n",
    "                            continue                               \n",
    "                        for entity in second_entity or []:\n",
    "                            if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                continue\n",
    "                            label = list(entity.keys())[0]       \n",
    "                            ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                WITH l\n",
    "                                MATCH (r:Point:`{ns_label}` {{id: $law_id2}})\n",
    "                                MERGE (r)-[:`{relation}`]->(l)\n",
    "                                \"\"\",\n",
    "                                law_id=label,\n",
    "                                law_id2=point_id\n",
    "                            )\n",
    "                    \n",
    "                    #Handle Subpoint\n",
    "                    for subpoint in point.get(\"subpoints\", []):\n",
    "                        subpoint_id = f\"{point_id}_SP_{subpoint.get('subpoint', '?')}\"\n",
    "                        subpoint_text = get_text(subpoint, \"text\")\n",
    "                        \n",
    "                        root = None\n",
    "                        ref = None\n",
    "                        second_entity = []\n",
    "                        \n",
    "                        dml_ddl_neo4j(\n",
    "                            f\"\"\"\n",
    "                            MERGE (sp:Subpoint:{ns_label} {{id: $id}})\n",
    "                            SET sp.original_text = $text\n",
    "                            SET sp.new_text = $new_text\n",
    "                            WITH sp\n",
    "                            MATCH (p:Point:{ns_label} {{id: $point_id}})\n",
    "                            MERGE (p)-[:HAS_SUBPOINT]->(sp)\n",
    "                            \"\"\",\n",
    "                            id=subpoint_id,\n",
    "                            text=subpoint_text,\n",
    "                            point_id=point_id,\n",
    "                            new_text = None\n",
    "                        )\n",
    "                        \n",
    "                        #Extract relation\n",
    "                        re_text = sent_tokenize(subpoint_text)\n",
    "                        for sentence in re_text:\n",
    "                            root = None\n",
    "                            re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                            if len(re_temp['document_id']) > 0:\n",
    "                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                            if 'nÃ y' in sentence.split():\n",
    "                                words = sentence.split()\n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                for i, token in enumerate(words):\n",
    "                                    if token == \"nÃ y\" and i > 0:\n",
    "                                        prev_word = words[i-1]\n",
    "                                        if prev_word.lower() in _check.keys():\n",
    "                                            text_type = _check[prev_word.lower()]\n",
    "                                            match text_type:\n",
    "                                                case \"doc1\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Luáº­t\"\n",
    "                                                case \"doc2\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                case \"doc3\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                case \"doc4\":\n",
    "                                                    root = metadata[\"law_id\"]\n",
    "                                                    ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                case \"chapter\":\n",
    "                                                    root = chapter_id\n",
    "                                                case \"clause\":\n",
    "                                                    root = clause_id\n",
    "                                                case \"point\":\n",
    "                                                    root = point_id\n",
    "                                            if root is not None:\n",
    "                                                _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                            else:\n",
    "                                                second_entity = []        \n",
    "                            if not second_entity or not relation:\n",
    "                                        continue                            \n",
    "                            for entity in second_entity or []:\n",
    "                                if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                    continue\n",
    "                                label = list(entity.keys())[0]       \n",
    "                                ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "\n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                    WITH l\n",
    "                                    MATCH (r:Subpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                    MERGE (r)-[:`{relation}`]->(l)\n",
    "                                    \"\"\",\n",
    "                                    law_id=label,\n",
    "                                    law_id2=subpoint_id\n",
    "                                )\n",
    "                        \n",
    "                        # Handle SubSubpoints (a.1))\n",
    "                        for ssp in subpoint.get(\"subsubpoints\", []):\n",
    "                            ssp_id = f\"{subpoint_id}_SSP_{ssp.get('subsubpoint', '?')}\"\n",
    "                            ssp_text = get_text(ssp, \"text\")\n",
    "                            \n",
    "                            root = None\n",
    "                            ref = None\n",
    "                            second_entity = []\n",
    "\n",
    "                            dml_ddl_neo4j(\n",
    "                                f\"\"\"\n",
    "                                MERGE (ssp:Subsubpoint:{ns_label} {{id: $id}})\n",
    "                                SET ssp.original_text = $text\n",
    "                                SET ssp.new_text = $new_text\n",
    "                                WITH ssp\n",
    "                                MATCH (sp:Subpoint:{ns_label} {{id: $subpoint_id}})\n",
    "                                MERGE (sp)-[:HAS_SUBSUBPOINT]->(ssp)\n",
    "                                \"\"\",\n",
    "                                id=ssp_id,\n",
    "                                text=ssp_text,\n",
    "                                subpoint_id=subpoint_id,\n",
    "                                new_text = None\n",
    "                            )\n",
    "                            \n",
    "                            #Extract relation\n",
    "                            re_text = sent_tokenize(ssp_text)\n",
    "                            for sentence in re_text:\n",
    "                                root = None\n",
    "                                re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                if len(re_temp['document_id']) > 0:\n",
    "                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                if 'nÃ y' in sentence.split():\n",
    "                                    words = sentence.split()\n",
    "                                    root = None\n",
    "                                    ref = None\n",
    "                                    for i, token in enumerate(words):\n",
    "                                        if token == \"nÃ y\" and i > 0:\n",
    "                                            prev_word = words[i-1]\n",
    "                                            if prev_word.lower() in _check.keys():\n",
    "                                                text_type = _check[prev_word.lower()]\n",
    "                                                match text_type:\n",
    "                                                    case \"doc1\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Luáº­t\"\n",
    "                                                    case \"doc2\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                    case \"doc3\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                    case \"doc4\":\n",
    "                                                        root = metadata[\"law_id\"]\n",
    "                                                        ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                    case \"chapter\":\n",
    "                                                        root = chapter_id\n",
    "                                                    case \"clause\":\n",
    "                                                        root = clause_id\n",
    "                                                    case \"point\":\n",
    "                                                        root = point_id\n",
    "                                                if root is not None:\n",
    "                                                    _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                else:\n",
    "                                                    second_entity = []        \n",
    "                                if not second_entity or not relation:\n",
    "                                        continue                              \n",
    "                                for entity in second_entity or []:\n",
    "                                    if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                        continue\n",
    "                                    label = list(entity.keys())[0]       \n",
    "                                    ref_type = ref if list(entity.values())[0] == 'Document' else list(entity.values())[0]\n",
    "                                    \n",
    "                                    dml_ddl_neo4j(\n",
    "                                        f\"\"\"\n",
    "                                        MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                        WITH l\n",
    "                                        MATCH (r:Subsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                        MERGE (r)-[:`{relation}`]->(l)\n",
    "                                        \"\"\",\n",
    "                                        law_id=label,\n",
    "                                        law_id2=ssp_id\n",
    "                                    )\n",
    "\n",
    "                            # Handle SubSubSubpoints (a.1.1))\n",
    "                            for sssp in ssp.get(\"subsubsubpoints\", []):\n",
    "                                sssp_id = f\"{ssp_id}_SSSP_{sssp.get('subsubsubpoint', '?')}\"\n",
    "                                sssp_text = get_text(sssp, \"text\")\n",
    "                                \n",
    "                                root = None\n",
    "                                ref = None\n",
    "                                second_entity = []\n",
    "\n",
    "                                dml_ddl_neo4j(\n",
    "                                    f\"\"\"\n",
    "                                    MERGE (sssp:Subsubsubpoint:{ns_label} {{id: $id}})\n",
    "                                    SET sssp.original_text = $text\n",
    "                                    SET sssp.new_text = $new_text\n",
    "                                    WITH sssp\n",
    "                                    MATCH (ssp:Subsubpoint:{ns_label} {{id: $ssp_id}})\n",
    "                                    MERGE (ssp)-[:HAS_SUBSUBSUBPOINT]->(sssp)\n",
    "                                    \"\"\",\n",
    "                                    id=sssp_id,\n",
    "                                    text=sssp_text,\n",
    "                                    ssp_id=ssp_id,\n",
    "                                    new_text = None\n",
    "                                )\n",
    "                                \n",
    "                                #Extract relation\n",
    "                                re_text = sent_tokenize(sssp_text)\n",
    "                                for sentence in re_text:\n",
    "                                    root = None\n",
    "                                    re_temp = final_re.final_relation(sentence)\n",
    "\n",
    "                                    if len(re_temp['document_id']) > 0:\n",
    "                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "\n",
    "                                    if 'nÃ y' in sentence.split():\n",
    "                                        words = sentence.split()\n",
    "                                        root = None\n",
    "                                        ref = None\n",
    "                                        for i, token in enumerate(words):\n",
    "                                            if token == \"nÃ y\" and i > 0:\n",
    "                                                prev_word = words[i-1]\n",
    "                                                if prev_word.lower() in _check.keys():\n",
    "                                                    text_type = _check[prev_word.lower()]\n",
    "                                                    match text_type:\n",
    "                                                        case \"doc1\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Luáº­t\"\n",
    "                                                        case \"doc2\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_Ä‘á»‹nh\"\n",
    "                                                        case \"doc3\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"ThÃ´ng_tÆ°\"\n",
    "                                                        case \"doc4\":\n",
    "                                                            root = metadata[\"law_id\"]\n",
    "                                                            ref = \"Nghá»‹_quyáº¿t\"\n",
    "                                                        case \"chapter\":\n",
    "                                                            root = chapter_id\n",
    "                                                        case \"clause\":\n",
    "                                                            root = clause_id\n",
    "                                                        case \"point\":\n",
    "                                                            root = point_id\n",
    "                                                    if root is not None:\n",
    "                                                        _, relation, second_entity = final_re.extract_relation_entities(sentence, root)\n",
    "                                                    else:\n",
    "                                                        second_entity = []        \n",
    "                                    if not second_entity or not relation:\n",
    "                                        continue                          \n",
    "                                    for entity in second_entity or []:\n",
    "                                        if (list(entity.keys())[0] == None) or list(entity.values())[0] is None:\n",
    "                                            continue\n",
    "                                        label = list(entity.keys())[0]       \n",
    "                                        ref_type = ref if list(entity.values())[0] == \"Document\" else list(entity.values())[0] \n",
    "\n",
    "                                        dml_ddl_neo4j(\n",
    "                                            f\"\"\"\n",
    "                                            MERGE (l:`{ref_type}`:`{ns_label}` {{id: $law_id}})\n",
    "                                            WITH l\n",
    "                                            MATCH (r:Subsubsubpoint:`{ns_label}` {{id: $law_id2}})\n",
    "                                            MERGE (r)-[:`{relation}`]->(l)\n",
    "                                            \"\"\",\n",
    "                                            law_id=label,\n",
    "                                            law_id2=sssp_id\n",
    "                                        )\n",
    "\n",
    "        # Cleanup temporary \"no_chapter\" node\n",
    "        dml_ddl_neo4j(\n",
    "            f\"\"\"\n",
    "            MATCH (ch:Chapter:{ns_label})\n",
    "            WHERE ch.id ENDS WITH \"_no_chapter\"\n",
    "            DETACH DELETE ch\n",
    "            \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a193a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 0 nodes, 0 rels in 15 ms.\n"
     ]
    }
   ],
   "source": [
    "# dml_ddl_neo4j('''\n",
    "#               match (n:Test_rel_3)\n",
    "#               detach delete n\n",
    "#               ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea379db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_neo4j(text, 'Test_rel_3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phobert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
